\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{lim2003planar}
\citation{peng2010novel}
\citation{biesiada2007feature}
\citation{ding2005minimum}
\citation{catal2009investigating}
\citation{liu2002comparative}
\citation{yu2003feature}
\citation{lim2003planar}
\citation{biesiada2007feature}
\citation{chen2006medical}
\citation{doi:10.1093/bib/bbx044}
\citation{chen2006medical}
\citation{blythe2008rise}
\citation{Turkay2014}
\citation{Holzinger2014}
\citation{dubitzky2007fundamentals}
\citation{PENG201015}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Automatic feature selection for high-dimensional biomedical data}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:FeatureSelection}{{1}{1}{Automatic feature selection for high-dimensional biomedical data}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}}
\newlabel{FSsec:Introduction}{{1.1}{1}{Introduction}{section.1.1}{}}
\citation{mckinney2010data}
\citation{yan2018hands}
\citation{walt2011NumPy}
\citation{jones2014SciPy}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces An overview of the symbols used to explain the data\relax }}{2}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:DatasetSymbols}{{1.1}{2}{An overview of the symbols used to explain the data\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Data Symbols}{2}{subsection.1.1.1}}
\newlabel{subsec:DataSymbols}{{1.1.1}{2}{Data Symbols}{subsection.1.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Software}{2}{section.1.2}}
\newlabel{PLsec:Software}{{1.2}{2}{Software}{section.1.2}{}}
\citation{pedregosa2011scikit}
\citation{mckinney2012Python}
\citation{olson2016tpot}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{brazma2001minimum}
\citation{selvaraj2011microarray}
\citation{afzal2015fast}
\citation{afzal2015fast}
\citation{selvaraj2011microarray}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Methods}{3}{section.1.3}}
\newlabel{FSsec:Background}{{1.3}{3}{Methods}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Datasets}{3}{subsection.1.3.1}}
\newlabel{FSsubsec:Datasets}{{1.3.1}{3}{Datasets}{subsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Micro-array Datasets}{3}{section*.3}}
\newlabel{PLsubsec:Microarray}{{1.3.1}{3}{Micro-array Datasets}{section*.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces A schematic overview of the four datasets.\relax }}{4}{table.caption.2}}
\newlabel{tab:DataSetDescriptions}{{1.2}{4}{A schematic overview of the four datasets.\relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A picture showing the creation of microarray datasets \cite  {afzal2015fast}.\relax }}{4}{figure.caption.4}}
\newlabel{fig:MicroArray}{{1.1}{4}{A picture showing the creation of microarray datasets \cite {afzal2015fast}.\relax }{figure.caption.4}{}}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{cottrell1999probability}
\citation{dettmer2007mass}
\citation{matthiesen2008analysis}
\citation{watson2007introduction}
\citation{neves2018mass}
\citation{neves2018mass}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{madigan2017brock}
\citation{Guyon2006}
\citation{CATAL20091040}
\citation{molina2002feature}
\citation{saeys2007review}
\@writefile{toc}{\contentsline {subsubsection}{Mass Spectrometry Datasets}{5}{section*.5}}
\newlabel{PLsubsec:MassSpect}{{1.3.1}{5}{Mass Spectrometry Datasets}{section*.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Four example figures of the data created by mass spectrometry. The measured molecule masses correspond to the peaks in the figures \cite  {neves2018mass}.\relax }}{6}{figure.caption.6}}
\newlabel{fig:MassSpect}{{1.2}{6}{Four example figures of the data created by mass spectrometry. The measured molecule masses correspond to the peaks in the figures \cite {neves2018mass}.\relax }{figure.caption.6}{}}
\citation{Duch2006}
\citation{saeys2007review}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{peng2005feature}
\citation{battiti1994using}
\citation{donoho2008higher}
\citation{storey2003statistical}
\citation{higgins2003measuring}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Feature Selection Methods}{7}{subsection.1.3.2}}
\newlabel{FSsubsec:FeatureSelectionMethods}{{1.3.2}{7}{Feature Selection Methods}{subsection.1.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Filter Methods}{7}{section*.7}}
\newlabel{FSsubsec:FilterMethods}{{1.3.2}{7}{Filter Methods}{section*.7}{}}
\newlabel{eq:MutualInformation}{{1.1}{7}{Filter Methods}{equation.1.3.1}{}}
\citation{Duch2006}
\citation{Duch2006}
\citation{Duch2006}
\citation{saeys2007review}
\citation{Reunanen2006}
\citation{Alsallakh2016PowerSet}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{tsamardinos2017massively}
\citation{huang2013automated}
\citation{saeys2007review}
\citation{SENAWI201747}
\citation{el2009new}
\citation{radovic2017minimum}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces A basic top $n$ filter algorithm \cite  {Duch2006}\relax }}{8}{algorithm.1}}
\newlabel{alg:FilterTopNAlgorithm}{{1}{8}{A basic top $n$ filter algorithm \cite {Duch2006}\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Wrapper Methods}{8}{section*.8}}
\newlabel{FSsubsec:WrapperMethods}{{1.3.2}{8}{Wrapper Methods}{section*.8}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces A forward selection sequential search algorithm \cite  {Reunanen2006}\relax }}{9}{algorithm.2}}
\newlabel{alg:ForwardSelection}{{2}{9}{A forward selection sequential search algorithm \cite {Reunanen2006}\relax }{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces A backward selection sequential search algorithm \cite  {Reunanen2006}\relax }}{9}{algorithm.3}}
\newlabel{alg:BackwardSelection}{{3}{9}{A backward selection sequential search algorithm \cite {Reunanen2006}\relax }{algorithm.3}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{saeys2007review}
\citation{Lal2006}
\citation{blum1997selection}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\@writefile{toc}{\contentsline {subsubsection}{Embedded Methods}{10}{section*.9}}
\newlabel{FSsubsec:EmbeddedMethods}{{1.3.2}{10}{Embedded Methods}{section*.9}{}}
\citation{jong2004feature}
\citation{prados2004mining}
\citation{zhang2006recursive}
\citation{guyon2002gene}
\citation{geurts2005proteomic}
\citation{wu2003comparison}
\citation{Duch2006}
\citation{liaw2002classification}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces An embedded forward selection algorithm \cite  {Lal2006}\relax }}{11}{algorithm.4}}
\newlabel{alg:EmbeddedForwardSelectionAlgorithm}{{4}{11}{An embedded forward selection algorithm \cite {Lal2006}\relax }{algorithm.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tree-based Pipeline Optimization Tool}{11}{section*.10}}
\newlabel{FSsubsec:TPOT}{{1.3.2}{11}{Tree-based Pipeline Optimization Tool}{section*.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Methods}{11}{section.1.4}}
\newlabel{FSsec:Methods}{{1.4}{11}{Methods}{section.1.4}{}}
\citation{hall1998practical}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Feature Selection Quality}{12}{subsection.1.4.1}}
\newlabel{FSsubsec:DimensionalityReductionQuality}{{1.4.1}{12}{Feature Selection Quality}{subsection.1.4.1}{}}
\newlabel{eq:FSAccuracy}{{1.4.1}{12}{Feature Selection Quality}{subsection.1.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces An example of the impact of the correction factor on the score, in this case accuracy. The shown correction factor uses $\beta = 0.99$. On the x-axis the number of features is shown and on the y-axis the value for the original accuracy, the correction factor and the \textit  {FS\_score}.\relax }}{13}{figure.caption.11}}
\newlabel{fig:FSAccuracyExample}{{1.3}{13}{An example of the impact of the correction factor on the score, in this case accuracy. The shown correction factor uses $\beta = 0.99$. On the x-axis the number of features is shown and on the y-axis the value for the original accuracy, the correction factor and the \textit {FS\_score}.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Feature Selection Exploration}{13}{subsection.1.4.2}}
\newlabel{FSsubsec:FeatureSelectionExploration}{{1.4.2}{13}{Feature Selection Exploration}{subsection.1.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Feature Selection Algorithms Evaluation}{13}{subsection.1.4.3}}
\newlabel{FSsubsec:FeatureSelectionAlgorithmsEvaluation}{{1.4.3}{13}{Feature Selection Algorithms Evaluation}{subsection.1.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces The four meta-parameters with their possible values in the first experiment.\relax }}{14}{table.caption.12}}
\newlabel{tab:FirstExperimentRequirements}{{1.3}{14}{The four meta-parameters with their possible values in the first experiment.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}TPOT Feature selection integration}{14}{subsection.1.4.4}}
\newlabel{FSsubsec:TPOTEvaluationIntegration}{{1.4.4}{14}{TPOT Feature selection integration}{subsection.1.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.4}{\ignorespaces The methods that are evaluated in the second experiment setup.\relax }}{15}{table.caption.13}}
\newlabel{tab:SecondExperimentMethods}{{1.4}{15}{The methods that are evaluated in the second experiment setup.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Results}{15}{section.1.5}}
\newlabel{FSsec:Results}{{1.5}{15}{Results}{section.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Feature Selection Exploration Results}{15}{subsection.1.5.1}}
\newlabel{FSsubsec:FeatureReductionExplorationResults}{{1.5.1}{15}{Feature Selection Exploration Results}{subsection.1.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.5}{\ignorespaces The experiment details for testing the non-trivial changes in \textit  {TPOT}. This experiment is rerun 5 times.\relax }}{16}{table.caption.14}}
\newlabel{tab:TPOTExpDetails}{{1.5}{16}{The experiment details for testing the non-trivial changes in \textit {TPOT}. This experiment is rerun 5 times.\relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces The average validation F1-scores shown per dataset and rank.\relax }}{16}{figure.caption.15}}
\newlabel{fig:DatasetRankF1Scores}{{1.4}{16}{The average validation F1-scores shown per dataset and rank.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Feature Selection Algorithms Evaluation Results}{16}{subsection.1.5.2}}
\newlabel{FSsubsec:FeatureSelectionAlgorithmsEvaluationResults}{{1.5.2}{16}{Feature Selection Algorithms Evaluation Results}{subsection.1.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces The F1 spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the F1 score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{18}{figure.caption.16}}
\newlabel{fig:Avg_F1_Spectrum}{{1.5}{18}{The F1 spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the F1 score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.6}{\ignorespaces The performance of the final pipeline for the different types of \textit  {TPOT} and the mentioned dataset. The five reruns are averaged into this one result.\relax }}{19}{table.caption.17}}
\newlabel{tab:TPOTResults}{{1.6}{19}{The performance of the final pipeline for the different types of \textit {TPOT} and the mentioned dataset. The five reruns are averaged into this one result.\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}TPOT Feature Selection Integration Results}{19}{subsection.1.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces The optimization process for the different \textit  {TPOT} algorithms for the micro-organisms dataset. - \relax $\@@underline {\hbox {TO DO: Make one for all 4 datasets}}\mathsurround \z@ $\relax \relax }}{19}{figure.caption.18}}
\newlabel{fig:TPOTResultMO}{{1.6}{19}{The optimization process for the different \textit {TPOT} algorithms for the micro-organisms dataset. - \underline {TO DO: Make one for all 4 datasets}\relax }{figure.caption.18}{}}
\citation{catal2009investigating}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{liu2002comparative}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Discussion}{20}{section.1.6}}
\newlabel{FSsec:Discussion}{{1.6}{20}{Discussion}{section.1.6}{}}
\bibdata{../References/Citings}
\bibcite{gehlenborg2010visualization}{1}
\bibcite{brazma2001minimum}{2}
\bibcite{cottrell1999probability}{3}
\bibcite{dettmer2007mass}{4}
\bibcite{liu2012data}{5}
\bibcite{sittig2008grand}{6}
\bibcite{magni1990chronic}{7}
\bibcite{bertolazzi2008logic}{8}
\bibcite{piatetsky2003microarray}{9}
\bibcite{lommen2009metalign}{10}
\bibcite{holzinger2014knowledge}{11}
\bibcite{wilkins2009proteomics}{12}
\bibcite{teodoro2009biomedical}{13}
\bibcite{doi:10.1093/nar/gkm1037}{14}
\bibcite{sturn2002genesis}{15}
\bibcite{karnovsky2011metscape}{16}
\bibcite{tabas2012genecodis3}{17}
\bibcite{faul2007g}{18}
\bibcite{baumgartner2006data}{19}
\bibcite{welthagen2005comprehensive}{20}
\bibcite{donders2006gentle}{21}
\bibcite{cartwright2003dealing}{22}
\bibcite{haukoos2007advanced}{23}
\bibcite{Deneer2017Thesis}{24}
\bibcite{nair2009genome}{25}
\bibcite{suarez2012expanding}{26}
\bibcite{bigler2013cross}{27}
\bibcite{kim2016spectrum}{28}
\bibcite{yao2008type}{29}
\bibcite{suarez2011nonlesional}{30}
\bibcite{tintle2011reversal}{31}
\bibcite{gittler2012progressive}{32}
\bibcite{CIOS20021}{33}
\bibcite{Yoo2012}{34}
\bibcite{chen2006medical}{35}
\bibcite{doi:10.1093/bib/bbx044}{36}
\bibcite{blythe2008rise}{37}
\bibcite{Turkay2014}{38}
\bibcite{Holzinger2014}{39}
\bibcite{dubitzky2007fundamentals}{40}
\bibcite{PENG201015}{41}
\bibcite{dunbar2006spatial}{42}
\bibcite{devroye1986sample}{43}
\bibcite{van2002gene}{44}
\bibcite{roff1989statistical}{45}
\bibcite{bellazzi2011data}{46}
\bibcite{Otasek2014}{47}
\bibcite{marenco2004qis}{48}
\bibcite{bichutskiy2006heterogeneous}{49}
\bibcite{sperzel1991biomedical}{50}
\bibcite{aubry1988design}{51}
\bibcite{Windridge2014}{52}
\bibcite{selvaraj2011microarray}{53}
\bibcite{afzal2015fast}{54}
\bibcite{wojnarski2010rsctc}{55}
\bibcite{matthiesen2008analysis}{56}
\bibcite{watson2007introduction}{57}
\bibcite{neves2018mass}{58}
\bibcite{NIPS2004_2728}{59}
\bibcite{doi:10.1093/bioinformatics/btu022}{60}
\bibcite{madigan2017brock}{61}
\bibcite{pocock2013clinical}{62}
\bibcite{bendele1999efficacy}{63}
\bibcite{kan1986short}{64}
\bibcite{diaconis1983computer}{65}
\bibcite{cestnikkononenkoj}{66}
\bibcite{murtaugh1994primary}{67}
\bibcite{misery2011sensitive}{68}
\bibcite{fernandes2017transfer}{69}
\bibcite{mckinney2010data}{70}
\bibcite{yan2018hands}{71}
\bibcite{walt2011NumPy}{72}
\bibcite{jones2014SciPy}{73}
\bibcite{pedregosa2011scikit}{74}
\bibcite{mckinney2012Python}{75}
\bibcite{olson2016tpot}{76}
\bibcite{yu2003feature}{77}
\bibcite{hall2000correlation}{78}
\bibcite{kohavi1995study}{79}
\bibcite{kearns1999algorithmic}{80}
\bibcite{agresti2003categorical}{81}
\bibcite{edwards2002explaining}{82}
\bibcite{han2011data}{83}
\bibcite{lim2003planar}{84}
\bibcite{biesiada2007feature}{85}
\bibcite{peng2005feature}{86}
\bibcite{battiti1994using}{87}
\bibcite{blackman2000interval}{88}
\bibcite{peng2010novel}{89}
\bibcite{ding2005minimum}{90}
\bibcite{catal2009investigating}{91}
\bibcite{liu2002comparative}{92}
\bibcite{Guyon2006}{93}
\bibcite{CATAL20091040}{94}
\bibcite{molina2002feature}{95}
\bibcite{saeys2007review}{96}
\bibcite{Duch2006}{97}
\bibcite{heiberger2004statistical}{98}
\bibcite{donoho2008higher}{99}
\bibcite{storey2003statistical}{100}
\bibcite{higgins2003measuring}{101}
\bibcite{Reunanen2006}{102}
\bibcite{Alsallakh2016PowerSet}{103}
\bibcite{tsamardinos2017massively}{104}
\bibcite{huang2013automated}{105}
\bibcite{SENAWI201747}{106}
\bibcite{el2009new}{107}
\bibcite{radovic2017minimum}{108}
\bibcite{kirkpatrick1983optimization}{109}
\bibcite{Jirapech-Umpai2005}{110}
\bibcite{Lal2006}{111}
\bibcite{blum1997selection}{112}
\bibcite{jong2004feature}{113}
\bibcite{prados2004mining}{114}
\bibcite{zhang2006recursive}{115}
\bibcite{guyon2002gene}{116}
\bibcite{geurts2005proteomic}{117}
\bibcite{wu2003comparison}{118}
\bibcite{liaw2002classification}{119}
\bibcite{thornton2013auto}{120}
\bibcite{Gijsbers2017Thesis}{121}
\bibcite{kotthoff2016auto}{122}
\bibcite{koza1997genetic}{123}
\bibcite{brazdil1994characterizing}{124}
\bibcite{vilalta2004using}{125}
\bibcite{brazdil2009development}{126}
\bibcite{pazzani1997learning}{127}
\bibcite{chen2006combining}{128}
\bibcite{hall1998practical}{129}
\bibstyle{ieeetr}
