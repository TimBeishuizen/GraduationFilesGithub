\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{gehlenborg2010visualization}
\citation{brazma2001minimum}
\citation{cottrell1999probability}
\citation{dettmer2007mass}
\citation{capitani2017nuclear}
\citation{liu2012data}
\citation{sittig2008grand}
\citation{gehlenborg2010visualization}
\citation{bertolazzi2008logic}
\citation{piatetsky2003microarray}
\citation{lommen2009metalign}
\citation{holzinger2014knowledge}
\citation{wilkins2009proteomics}
\citation{teodoro2009biomedical}
\citation{doi:10.1093/nar/gkm1037}
\citation{sturn2002genesis}
\citation{karnovsky2011metscape}
\citation{tabas2012genecodis3}
\citation{faul2007g}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{lim2003planar}
\citation{peng2010novel}
\citation{biesiada2007feature}
\citation{ding2005minimum}
\citation{catal2009investigating}
\citation{liu2002comparative}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{OpenML2013}
\citation{felix2017dynamic}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:Introduction}{{1}{2}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}}
\newlabel{sec:Background}{{2}{2}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Datasets}{2}{subsection.2.1}}
\newlabel{subsec:Datasets}{{2.1}{2}{Datasets}{subsection.2.1}{}}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{madigan2017brock}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{Guyon2006}
\citation{CATAL20091040}
\citation{molina2002feature}
\citation{chandrashekar2014survey}
\citation{saeys2007review}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A schematic overview of the four datasets.\relax }}{3}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:DataSetDescriptions}{{1}{3}{A schematic overview of the four datasets.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Feature Selection}{3}{subsection.2.2}}
\newlabel{subsec:FeatureSelection}{{2.2}{3}{Feature Selection}{subsection.2.2}{}}
\citation{Duch2006}
\citation{saeys2007review}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{jones2014scipy}
\citation{peng2005feature}
\citation{battiti1994using}
\citation{pedregosa2011scikit}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Filter Methods}{4}{subsubsection.2.2.1}}
\newlabel{subsec:FilterMethods}{{2.2.1}{4}{Filter Methods}{subsubsection.2.2.1}{}}
\citation{yu2003feature}
\citation{hall2000correlation}
\citation{pedregosa2011scikit}
\citation{donoho2008higher}
\citation{storey2003statistical}
\citation{higgins2003measuring}
\citation{Duch2006}
\citation{Duch2006}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces T-test formulas to compute whether two samples are independent by means. The parameters $s_{\Delta \mathaccentV {bar}016{y}}$ and $t_{calc}$ can be three different values (Table \ref  {tab:sdyandtcalc})\cite  {heiberger2004statistical}. This is calculated with the values for the mean $\mu $, significance level $\alpha $, average $\mathaccentV {bar}016{y}$.\relax }}{5}{table.caption.3}}
\newlabel{tab:IndepTest}{{2}{5}{T-test formulas to compute whether two samples are independent by means. The parameters $s_{\Delta \bar {y}}$ and $t_{calc}$ can be three different values (Table \ref {tab:sdyandtcalc})\cite {heiberger2004statistical}. This is calculated with the values for the mean $\mu $, significance level $\alpha $, average $\bar {y}$.\relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The values of $s_{\Delta \mathaccentV {bar}016{y}}$ and $t_{calc}$ for data sets with common unknown variance, uncommon unknown variance and paired data. \cite  {heiberger2004statistical}. This is calculated with the values for the average $\mathaccentV {bar}016{y}$, variance $s$, population size $n$ and average paired data difference $\mathaccentV {hat}05E{d}$.\relax }}{5}{table.caption.4}}
\newlabel{tab:sdyandtcalc}{{3}{5}{The values of $s_{\Delta \bar {y}}$ and $t_{calc}$ for data sets with common unknown variance, uncommon unknown variance and paired data. \cite {heiberger2004statistical}. This is calculated with the values for the average $\bar {y}$, variance $s$, population size $n$ and average paired data difference $\hat {d}$.\relax }{table.caption.4}{}}
\newlabel{eq:MutualInformation}{{1}{5}{Filter Methods}{equation.2.1}{}}
\newlabel{eq:Correlation}{{2}{5}{Filter Methods}{equation.2.2}{}}
\citation{Duch2006}
\citation{Duch2006}
\citation{Duch2006}
\citation{saeys2007review}
\citation{Reunanen2006}
\citation{Alsallakh2016PowerSet}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{tsamardinos2017massively}
\citation{huang2013automated}
\citation{saeys2007review}
\citation{zhang2004optimality}
\citation{SENAWI201747}
\citation{el2009new}
\citation{radovic2017minimum}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces A basic top $n$ filter algorithm\cite  {Duch2006}\relax }}{6}{algorithm.1}}
\newlabel{alg:FilterTopNAlgorithm}{{1}{6}{A basic top $n$ filter algorithm\cite {Duch2006}\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces A basic filter algorithm\cite  {Duch2006}\relax }}{6}{algorithm.2}}
\newlabel{alg:FilterAlgorithm}{{2}{6}{A basic filter algorithm\cite {Duch2006}\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Wrapper Methods}{6}{subsubsection.2.2.2}}
\newlabel{subsec:WrapperMethods}{{2.2.2}{6}{Wrapper Methods}{subsubsection.2.2.2}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces A forward selection sequential search algorithm\cite  {Reunanen2006}\relax }}{7}{algorithm.3}}
\newlabel{alg:ForwardSelection}{{3}{7}{A forward selection sequential search algorithm\cite {Reunanen2006}\relax }{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces A backward selection sequential search algorithm\cite  {Reunanen2006}\relax }}{7}{algorithm.4}}
\newlabel{alg:BackwardSelection}{{4}{7}{A backward selection sequential search algorithm\cite {Reunanen2006}\relax }{algorithm.4}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{kirkpatrick1983optimization}
\citation{Reunanen2006}
\citation{Jirapech-Umpai2005}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces A plus l-take away r sequential search algorithm\cite  {Reunanen2006}\relax }}{8}{algorithm.5}}
\newlabel{alg:PTA}{{5}{8}{A plus l-take away r sequential search algorithm\cite {Reunanen2006}\relax }{algorithm.5}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces A floating search algorithm\cite  {Reunanen2006}\relax }}{9}{algorithm.6}}
\newlabel{alg:FloatingSearch}{{6}{9}{A floating search algorithm\cite {Reunanen2006}\relax }{algorithm.6}{}}
\citation{Reunanen2006}
\citation{saeys2007review}
\citation{Lal2006}
\citation{blum1997selection}
\citation{Lal2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Simulated Annealing search algorithm\cite  {Reunanen2006}\relax }}{10}{algorithm.7}}
\newlabel{alg:SASearch}{{7}{10}{Simulated Annealing search algorithm\cite {Reunanen2006}\relax }{algorithm.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Embedded Methods}{10}{subsubsection.2.2.3}}
\newlabel{subsec:EmbeddedMethods}{{2.2.3}{10}{Embedded Methods}{subsubsection.2.2.3}{}}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces An embedded forward selection algorithm\cite  {Lal2006}\relax }}{11}{algorithm.8}}
\newlabel{alg:EmbeddedForwardSelectionAlgorithm}{{8}{11}{An embedded forward selection algorithm\cite {Lal2006}\relax }{algorithm.8}{}}
\citation{jong2004feature}
\citation{prados2004mining}
\citation{zhang2006recursive}
\citation{guyon2002gene}
\citation{geurts2005proteomic}
\citation{wu2003comparison}
\citation{Duch2006}
\citation{liaw2002classification}
\citation{pedregosa2011scikit}
\citation{jones2014scipy}
\citation{walt2011numpy}
\citation{hall1998practical}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces An embedded backward elimination algorithm\cite  {Lal2006} \relax }}{12}{algorithm.9}}
\newlabel{alg:EmbeddedBackwardEliminationAlgorithm}{{9}{12}{An embedded backward elimination algorithm\cite {Lal2006} \relax }{algorithm.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{12}{section.3}}
\newlabel{sec:Methods}{{3}{12}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Feature Selection Quality}{12}{subsection.3.1}}
\newlabel{subsec:DimensionalityReductionQuality}{{3.1}{12}{Feature Selection Quality}{subsection.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The four meta-parameters with their possible values in the first experiment.\relax }}{13}{table.caption.5}}
\newlabel{tab:FirstExperimentRequirements}{{4}{13}{The four meta-parameters with their possible values in the first experiment.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Feature Selection Exploration}{13}{subsection.3.2}}
\newlabel{subsec:FeatureSelectionExploration}{{3.2}{13}{Feature Selection Exploration}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Feature Selection Algorithms Evaluation}{13}{subsection.3.3}}
\newlabel{subsec:FeatureSelectionAlgorithmsEvaluation}{{3.3}{13}{Feature Selection Algorithms Evaluation}{subsection.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The methods that are evaluated in the second experiment setup.\relax }}{14}{table.caption.6}}
\newlabel{tab:SecondExperimentMethods}{{5}{14}{The methods that are evaluated in the second experiment setup.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{14}{section.4}}
\newlabel{sec:Results}{{4}{14}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Selection Exploration Results}{14}{subsection.4.1}}
\newlabel{subsec:FeatureReductionExplorationResults}{{4.1}{14}{Feature Selection Exploration Results}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The average validation and test scores after averaging the scores for the data sets and ranking methods.\relax }}{15}{figure.caption.7}}
\newlabel{fig:ValTestScores}{{1}{15}{The average validation and test scores after averaging the scores for the data sets and ranking methods.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The average validation scores shown per machine learning quality measurement.\relax }}{15}{figure.caption.8}}
\newlabel{fig:MachineLearningQualityScores}{{2}{15}{The average validation scores shown per machine learning quality measurement.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The average validation scores shown per dataset and rank.\relax }}{16}{figure.caption.9}}
\newlabel{fig:DatasetRankScores}{{3}{16}{The average validation scores shown per dataset and rank.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The average F1-scores shown per dataset and rank.\relax }}{17}{figure.caption.10}}
\newlabel{fig:DatasetRankF1Scores}{{4}{17}{The average F1-scores shown per dataset and rank.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feature Selection Algorithms Evaluation Results}{17}{subsection.4.2}}
\newlabel{subsec:FeatureSelectionAlgorithmsEvaluationResults}{{4.2}{17}{Feature Selection Algorithms Evaluation Results}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The accuracy spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the accuracy of logistic regression. The legend indicates the algorithms (Table \ref  {tab:SecondExperimentMethods}) and their matching shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{18}{figure.caption.11}}
\newlabel{fig:Avg_Accuracy_Spectrum}{{5}{18}{The accuracy spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the accuracy of logistic regression. The legend indicates the algorithms (Table \ref {tab:SecondExperimentMethods}) and their matching shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The precision spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the precision of logistic regression. The legend indicates the algorithms and their matching shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{19}{figure.caption.12}}
\newlabel{fig:Avg_Precision_Spectrum}{{6}{19}{The precision spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the precision of logistic regression. The legend indicates the algorithms and their matching shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The recall spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the recall of logistic regression. The legend indicates the algorithms and their matching shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{20}{figure.caption.13}}
\newlabel{fig:Avg_Recall_Spectrum}{{7}{20}{The recall spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the recall of logistic regression. The legend indicates the algorithms and their matching shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The F1 spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the F1 score of logistic regression. The legend indicates the algorithms and their matching shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{21}{figure.caption.14}}
\newlabel{fig:Avg_F1_Spectrum}{{8}{21}{The F1 spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the F1 score of logistic regression. The legend indicates the algorithms and their matching shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A bar chart showing the average computation time of all evaluated algorithms. The x-axis corresponded to the different algorithms, for which the computation time for multiple parameter combinations were given. The y-axis corresponded to the computation time in seconds. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Support Vector Machine (svm), random forest (rf)\relax }}{23}{figure.caption.15}}
\newlabel{fig:Comp_Time_Bar}{{9}{23}{A bar chart showing the average computation time of all evaluated algorithms. The x-axis corresponded to the different algorithms, for which the computation time for multiple parameter combinations were given. The y-axis corresponded to the computation time in seconds. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.15}{}}
\citation{catal2009investigating}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{liu2002comparative}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{24}{section.5}}
\newlabel{sec:Discussion}{{5}{24}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{24}{section.6}}
\newlabel{sec:Conclusions}{{6}{24}{Conclusions}{section.6}{}}
\bibdata{../References/Citings}
\bibcite{gehlenborg2010visualization}{1}
\bibcite{brazma2001minimum}{2}
\bibcite{cottrell1999probability}{3}
\bibcite{dettmer2007mass}{4}
\bibcite{capitani2017nuclear}{5}
\bibcite{liu2012data}{6}
\bibcite{sittig2008grand}{7}
\bibcite{bertolazzi2008logic}{8}
\bibcite{piatetsky2003microarray}{9}
\bibcite{lommen2009metalign}{10}
\bibcite{holzinger2014knowledge}{11}
\bibcite{wilkins2009proteomics}{12}
\bibcite{teodoro2009biomedical}{13}
\bibcite{doi:10.1093/nar/gkm1037}{14}
\bibcite{sturn2002genesis}{15}
\bibcite{karnovsky2011metscape}{16}
\bibcite{tabas2012genecodis3}{17}
\bibcite{faul2007g}{18}
\bibcite{baumgartner2006data}{19}
\bibcite{welthagen2005comprehensive}{20}
\bibcite{lim2003planar}{21}
\bibcite{peng2010novel}{22}
\bibcite{biesiada2007feature}{23}
\bibcite{ding2005minimum}{24}
\bibcite{catal2009investigating}{25}
\bibcite{liu2002comparative}{26}
\bibcite{nair2009genome}{27}
\bibcite{suarez2012expanding}{28}
\bibcite{bigler2013cross}{29}
\bibcite{yao2008type}{30}
\bibcite{wojnarski2010rsctc}{31}
\bibcite{NIPS2004_2728}{32}
\bibcite{doi:10.1093/bioinformatics/btu022}{33}
\bibcite{OpenML2013}{34}
\bibcite{felix2017dynamic}{35}
\bibcite{madigan2017brock}{36}
\bibcite{Guyon2006}{37}
\bibcite{CATAL20091040}{38}
\bibcite{molina2002feature}{39}
\bibcite{chandrashekar2014survey}{40}
\bibcite{saeys2007review}{41}
\bibcite{Duch2006}{42}
\bibcite{heiberger2004statistical}{43}
\bibcite{jones2014scipy}{44}
\bibcite{peng2005feature}{45}
\bibcite{battiti1994using}{46}
\bibcite{pedregosa2011scikit}{47}
\bibcite{yu2003feature}{48}
\bibcite{hall2000correlation}{49}
\bibcite{donoho2008higher}{50}
\bibcite{storey2003statistical}{51}
\bibcite{higgins2003measuring}{52}
\bibcite{Reunanen2006}{53}
\bibcite{Alsallakh2016PowerSet}{54}
\bibcite{tsamardinos2017massively}{55}
\bibcite{huang2013automated}{56}
\bibcite{zhang2004optimality}{57}
\bibcite{SENAWI201747}{58}
\bibcite{el2009new}{59}
\bibcite{radovic2017minimum}{60}
\bibcite{kirkpatrick1983optimization}{61}
\bibcite{Jirapech-Umpai2005}{62}
\bibcite{Lal2006}{63}
\bibcite{blum1997selection}{64}
\bibcite{jong2004feature}{65}
\bibcite{prados2004mining}{66}
\bibcite{zhang2006recursive}{67}
\bibcite{guyon2002gene}{68}
\bibcite{geurts2005proteomic}{69}
\bibcite{wu2003comparison}{70}
\bibcite{liaw2002classification}{71}
\bibcite{walt2011numpy}{72}
\bibcite{hall1998practical}{73}
\bibstyle{ieeetr}
\@writefile{toc}{\contentsline {section}{\numberline {A}Feature Selection Exploration Plots}{29}{appendix.A}}
\newlabel{app:FeatureSelectionExplorationPlots}{{A}{29}{Feature Selection Exploration Plots}{appendix.A}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The validation and test score per feature preserved for the micro organisms data setwith ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{30}{figure.caption.17}}
\newlabel{fig:MO_MI_Val_Test_Score}{{10}{30}{The validation and test score per feature preserved for the micro organisms data setwith ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The validation and test score per feature preserved for the micro organisms dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{30}{figure.caption.18}}
\newlabel{fig:MO_T_Val_Test_Score}{{11}{30}{The validation and test score per feature preserved for the micro organisms dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The validation and test score per feature preserved for the Arcene dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{31}{figure.caption.19}}
\newlabel{fig:Arcene_MI_Val_Test_Score}{{12}{31}{The validation and test score per feature preserved for the Arcene dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The validation and test score per feature preserved for the Arcene dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{31}{figure.caption.20}}
\newlabel{fig:Arcene_T_Val_Test_Score}{{13}{31}{The validation and test score per feature preserved for the Arcene dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The validation and test score per feature preserved for the RSCTC dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{32}{figure.caption.21}}
\newlabel{fig:RSCTC_MI_Val_Test_Score}{{14}{32}{The validation and test score per feature preserved for the RSCTC dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The validation and test score per feature preserved for the RSCTC dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{32}{figure.caption.22}}
\newlabel{fig:RSCTC_T_Val_Test_Score}{{15}{32}{The validation and test score per feature preserved for the RSCTC dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The validation and test score per feature preserved for the Psoriasis dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{33}{figure.caption.23}}
\newlabel{fig:Psoriasis_MI_Val_Test_Score}{{16}{33}{The validation and test score per feature preserved for the Psoriasis dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The validation and test score per feature preserved for the Psoriasis dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{33}{figure.caption.24}}
\newlabel{fig:Psoriasis_T_Val_Test_Score}{{17}{33}{The validation and test score per feature preserved for the Psoriasis dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Feature Selection Exploration Precision And Recall}{33}{appendix.B}}
\newlabel{app:PrecisionRecall}{{B}{33}{Feature Selection Exploration Precision And Recall}{appendix.B}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The average precision shown per dataset and rank.\relax }}{34}{figure.caption.25}}
\newlabel{fig:DatasetRankPrecisionScores}{{18}{34}{The average precision shown per dataset and rank.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The average recall shown per dataset and rank.\relax }}{34}{figure.caption.26}}
\newlabel{fig:DatasetRankRecallScores}{{19}{34}{The average recall shown per dataset and rank.\relax }{figure.caption.26}{}}
