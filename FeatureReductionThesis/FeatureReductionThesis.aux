\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{gehlenborg2010visualization}
\citation{brazma2001minimum}
\citation{cottrell1999probability}
\citation{dettmer2007mass}
\citation{capitani2017nuclear}
\citation{liu2012data}
\citation{sittig2008grand}
\citation{gehlenborg2010visualization}
\citation{bertolazzi2008logic}
\citation{piatetsky2003microarray}
\citation{lommen2009metalign}
\citation{holzinger2014knowledge}
\citation{wilkins2009proteomics}
\citation{teodoro2009biomedical}
\citation{doi:10.1093/nar/gkm1037}
\citation{sturn2002genesis}
\citation{karnovsky2011metscape}
\citation{tabas2012genecodis3}
\citation{faul2007g}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{lim2003planar}
\citation{peng2010novel}
\citation{biesiada2007feature}
\citation{ding2005minimum}
\citation{catal2009investigating}
\citation{liu2002comparative}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:Introduction}{{1}{2}{Introduction}{section.1}{}}
\citation{OpenML2013}
\citation{felix2017dynamic}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{madigan2017brock}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{Guyon2006}
\citation{CATAL20091040}
\citation{molina2002feature}
\citation{chandrashekar2014survey}
\citation{saeys2007review}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}{section.2}}
\newlabel{sec:Background}{{2}{3}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Datasets}{3}{subsection.2.1}}
\newlabel{subsec:Datasets}{{2.1}{3}{Datasets}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Feature Selection}{3}{subsection.2.2}}
\newlabel{subsec:FeatureSelection}{{2.2}{3}{Feature Selection}{subsection.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A schematic overview of the four datasets.\relax }}{4}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:DataSetDescriptions}{{1}{4}{A schematic overview of the four datasets.\relax }{table.caption.2}{}}
\citation{Duch2006}
\citation{saeys2007review}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{jones2014scipy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Filter Methods}{5}{subsubsection.2.2.1}}
\newlabel{subsec:FilterMethods}{{2.2.1}{5}{Filter Methods}{subsubsection.2.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces T-test formulas to compute whether two samples are independent by means. The parameters $s_{\Delta \mathaccentV {bar}016{y}}$ and $t_{calc}$ can be three different values (Table \ref  {tab:sdyandtcalc})\cite  {heiberger2004statistical}. This is calculated with the values for the mean $\mu $, significance level $\alpha $, average $\mathaccentV {bar}016{y}$.\relax }}{5}{table.caption.3}}
\newlabel{tab:IndepTest}{{2}{5}{T-test formulas to compute whether two samples are independent by means. The parameters $s_{\Delta \bar {y}}$ and $t_{calc}$ can be three different values (Table \ref {tab:sdyandtcalc})\cite {heiberger2004statistical}. This is calculated with the values for the mean $\mu $, significance level $\alpha $, average $\bar {y}$.\relax }{table.caption.3}{}}
\citation{peng2005feature}
\citation{battiti1994using}
\citation{pedregosa2011scikit}
\citation{yu2003feature}
\citation{hall2000correlation}
\citation{pedregosa2011scikit}
\citation{donoho2008higher}
\citation{storey2003statistical}
\citation{higgins2003measuring}
\citation{Duch2006}
\citation{Duch2006}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The values of $s_{\Delta \mathaccentV {bar}016{y}}$ and $t_{calc}$ for data sets with common unknown variance, uncommon unknown variance and paired data. \cite  {heiberger2004statistical}. This is calculated with the values for the average $\mathaccentV {bar}016{y}$, variance $s$, population size $n$ and average paired data difference $\mathaccentV {hat}05E{d}$.\relax }}{6}{table.caption.4}}
\newlabel{tab:sdyandtcalc}{{3}{6}{The values of $s_{\Delta \bar {y}}$ and $t_{calc}$ for data sets with common unknown variance, uncommon unknown variance and paired data. \cite {heiberger2004statistical}. This is calculated with the values for the average $\bar {y}$, variance $s$, population size $n$ and average paired data difference $\hat {d}$.\relax }{table.caption.4}{}}
\newlabel{eq:MutualInformation}{{1}{6}{Filter Methods}{equation.2.1}{}}
\newlabel{eq:Correlation}{{2}{6}{Filter Methods}{equation.2.2}{}}
\citation{Duch2006}
\citation{Duch2006}
\citation{Duch2006}
\citation{saeys2007review}
\citation{Reunanen2006}
\citation{Alsallakh2016PowerSet}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{tsamardinos2017massively}
\citation{huang2013automated}
\citation{saeys2007review}
\citation{zhang2004optimality}
\citation{SENAWI201747}
\citation{el2009new}
\citation{radovic2017minimum}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces A basic top $n$ filter algorithm\cite  {Duch2006}\relax }}{7}{algorithm.1}}
\newlabel{alg:FilterTopNAlgorithm}{{1}{7}{A basic top $n$ filter algorithm\cite {Duch2006}\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces A basic filter algorithm\cite  {Duch2006}\relax }}{7}{algorithm.2}}
\newlabel{alg:FilterAlgorithm}{{2}{7}{A basic filter algorithm\cite {Duch2006}\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Wrapper Methods}{7}{subsubsection.2.2.2}}
\newlabel{subsec:WrapperMethods}{{2.2.2}{7}{Wrapper Methods}{subsubsection.2.2.2}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces A forward selection sequential search algorithm\cite  {Reunanen2006}\relax }}{8}{algorithm.3}}
\newlabel{alg:ForwardSelection}{{3}{8}{A forward selection sequential search algorithm\cite {Reunanen2006}\relax }{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces A backward selection sequential search algorithm\cite  {Reunanen2006}\relax }}{8}{algorithm.4}}
\newlabel{alg:BackwardSelection}{{4}{8}{A backward selection sequential search algorithm\cite {Reunanen2006}\relax }{algorithm.4}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{kirkpatrick1983optimization}
\citation{Reunanen2006}
\citation{Jirapech-Umpai2005}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces A plus l-take away r sequential search algorithm\cite  {Reunanen2006}\relax }}{9}{algorithm.5}}
\newlabel{alg:PTA}{{5}{9}{A plus l-take away r sequential search algorithm\cite {Reunanen2006}\relax }{algorithm.5}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces A floating search algorithm\cite  {Reunanen2006}\relax }}{10}{algorithm.6}}
\newlabel{alg:FloatingSearch}{{6}{10}{A floating search algorithm\cite {Reunanen2006}\relax }{algorithm.6}{}}
\citation{Reunanen2006}
\citation{saeys2007review}
\citation{Lal2006}
\citation{blum1997selection}
\citation{Lal2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Simulated Annealing search algorithm\cite  {Reunanen2006}\relax }}{11}{algorithm.7}}
\newlabel{alg:SASearch}{{7}{11}{Simulated Annealing search algorithm\cite {Reunanen2006}\relax }{algorithm.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Embedded Methods}{11}{subsubsection.2.2.3}}
\newlabel{subsec:EmbeddedMethods}{{2.2.3}{11}{Embedded Methods}{subsubsection.2.2.3}{}}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces An embedded forward selection algorithm\cite  {Lal2006}\relax }}{12}{algorithm.8}}
\newlabel{alg:EmbeddedForwardSelectionAlgorithm}{{8}{12}{An embedded forward selection algorithm\cite {Lal2006}\relax }{algorithm.8}{}}
\citation{jong2004feature}
\citation{prados2004mining}
\citation{zhang2006recursive}
\citation{guyon2002gene}
\citation{geurts2005proteomic}
\citation{wu2003comparison}
\citation{Duch2006}
\citation{liaw2002classification}
\citation{thornton2013auto}
\citation{Gijsbers2017Thesis}
\citation{thornton2013auto}
\citation{kotthoff2016auto}
\citation{Gijsbers2017Thesis}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces An embedded backward elimination algorithm\cite  {Lal2006} \relax }}{13}{algorithm.9}}
\newlabel{alg:EmbeddedBackwardEliminationAlgorithm}{{9}{13}{An embedded backward elimination algorithm\cite {Lal2006} \relax }{algorithm.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Automated Machine Learning}{13}{subsection.2.3}}
\newlabel{subsec:AutomatedMachineLearning}{{2.3}{13}{Automated Machine Learning}{subsection.2.3}{}}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\citation{brazdil1994characterizing}
\citation{vilalta2004using}
\citation{brazdil2009development}
\newlabel{fig:Meta-LearningLayout}{{\caption@xref {fig:Meta-LearningLayout}{ on input line 581}}{14}{Meta-Learning}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A layout of how meta-learning works. 1. Data sets are collected. 2. Meta-data is computed for each dataset. 3. A meta-dataset is created and a meta-model is learned\cite  {Gijsbers2017Thesis}.\relax }}{14}{figure.caption.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Meta-Learning}{14}{subsubsection.2.3.1}}
\newlabel{subsec:Meta-Learning}{{2.3.1}{14}{Meta-Learning}{subsubsection.2.3.1}{}}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces All TPOT Algorithms used for the TPOT classifier function. Not only the algorithms differ, but also the hyper-parameters within these algorithms.\relax }}{15}{table.caption.6}}
\newlabel{tab:TPOTAlgorithms}{{4}{15}{All TPOT Algorithms used for the TPOT classifier function. Not only the algorithms differ, but also the hyper-parameters within these algorithms.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Tree-based Pipeline Optimization Tool}{15}{subsubsection.2.3.2}}
\newlabel{subsec:TPOT}{{2.3.2}{15}{Tree-based Pipeline Optimization Tool}{subsubsection.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example of a machine learning pipeline in TPOT. It only shows the primitive algorithms and not hyperparameter terminals. At the root is the machine learning algorithm\cite  {Gijsbers2017Thesis}.\relax }}{15}{figure.caption.7}}
\newlabel{fig:MachineLearningPipeline}{{2}{15}{An example of a machine learning pipeline in TPOT. It only shows the primitive algorithms and not hyperparameter terminals. At the root is the machine learning algorithm\cite {Gijsbers2017Thesis}.\relax }{figure.caption.7}{}}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Examples of the three mutations in the TPOT algorithm: insertion, replacement and shrinking\cite  {Gijsbers2017Thesis}.\relax }}{16}{figure.caption.8}}
\newlabel{fig:TPOTMutations}{{3}{16}{Examples of the three mutations in the TPOT algorithm: insertion, replacement and shrinking\cite {Gijsbers2017Thesis}.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example of a TPOT crossover\cite  {Gijsbers2017Thesis}.\relax }}{16}{figure.caption.9}}
\newlabel{fig:TPOTCrossover}{{4}{16}{An example of a TPOT crossover\cite {Gijsbers2017Thesis}.\relax }{figure.caption.9}{}}
\citation{pazzani1997learning}
\citation{chen2006combining}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hypotheses}{18}{section.3}}
\newlabel{sec:hypotheses}{{3}{18}{Hypotheses}{section.3}{}}
\citation{pedregosa2011scikit}
\citation{jones2014scipy}
\citation{walt2011numpy}
\citation{hall1998practical}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{19}{section.4}}
\newlabel{sec:Methods}{{4}{19}{Methods}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Selection Quality}{19}{subsection.4.1}}
\newlabel{subsec:DimensionalityReductionQuality}{{4.1}{19}{Feature Selection Quality}{subsection.4.1}{}}
\newlabel{eq:FSAccuracy}{{4.1}{19}{Feature Selection Quality}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The correction factor for the FS\_accuracy from Equation \ref  {eq:FSAccuracy}. Every graph corresponds to a different value of $\alpha $.\relax }}{20}{figure.caption.10}}
\newlabel{fig:FSAccuracyPlot}{{5}{20}{The correction factor for the FS\_accuracy from Equation \ref {eq:FSAccuracy}. Every graph corresponds to a different value of $\alpha $.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An example of the impact of the correction factor on the accuracy. The shown correction factor uses $\alpha = 0.99$. On the x-axis the number of features is shown and on the y-axis the value for the original accuracy, the correction factor and the FS\_accuracy.\relax }}{21}{figure.caption.11}}
\newlabel{fig:FSAccuracyExample}{{6}{21}{An example of the impact of the correction factor on the accuracy. The shown correction factor uses $\alpha = 0.99$. On the x-axis the number of features is shown and on the y-axis the value for the original accuracy, the correction factor and the FS\_accuracy.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feature Selection Exploration}{21}{subsection.4.2}}
\newlabel{subsec:FeatureSelectionExploration}{{4.2}{21}{Feature Selection Exploration}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Feature Selection Algorithms Evaluation}{21}{subsection.4.3}}
\newlabel{subsec:FeatureSelectionAlgorithmsEvaluation}{{4.3}{21}{Feature Selection Algorithms Evaluation}{subsection.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The four meta-parameters with their possible values in the first experiment.\relax }}{22}{table.caption.12}}
\newlabel{tab:FirstExperimentRequirements}{{5}{22}{The four meta-parameters with their possible values in the first experiment.\relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The methods that are evaluated in the second experiment setup.\relax }}{23}{table.caption.13}}
\newlabel{tab:SecondExperimentMethods}{{6}{23}{The methods that are evaluated in the second experiment setup.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}TPOT Feature selection integration}{23}{subsection.4.4}}
\newlabel{subsec:TPOTEvaluationIntegration}{{4.4}{23}{TPOT Feature selection integration}{subsection.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The experiment details for testing the non-trivial changes in TPOT.\relax }}{25}{table.caption.14}}
\newlabel{tab:TPOTExpDetails}{{7}{25}{The experiment details for testing the non-trivial changes in TPOT.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{26}{section.5}}
\newlabel{sec:Results}{{5}{26}{Results}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Feature Selection Exploration Results}{26}{subsection.5.1}}
\newlabel{subsec:FeatureReductionExplorationResults}{{5.1}{26}{Feature Selection Exploration Results}{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The average validation and test scores after averaging the scores for the data sets and ranking methods.\relax }}{26}{figure.caption.15}}
\newlabel{fig:ValTestScores}{{7}{26}{The average validation and test scores after averaging the scores for the data sets and ranking methods.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The average validation scores shown per machine learning quality measurement.\relax }}{27}{figure.caption.16}}
\newlabel{fig:MachineLearningQualityScores}{{8}{27}{The average validation scores shown per machine learning quality measurement.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The average validation scores shown per dataset and rank.\relax }}{27}{figure.caption.17}}
\newlabel{fig:DatasetRankScores}{{9}{27}{The average validation scores shown per dataset and rank.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The average F1-scores shown per dataset and rank.\relax }}{28}{figure.caption.18}}
\newlabel{fig:DatasetRankF1Scores}{{10}{28}{The average F1-scores shown per dataset and rank.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Feature Selection Algorithms Evaluation Results}{28}{subsection.5.2}}
\newlabel{subsec:FeatureSelectionAlgorithmsEvaluationResults}{{5.2}{28}{Feature Selection Algorithms Evaluation Results}{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The accuracy spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the accuracy of logistic regression. The legend indicates the algorithms (Table \ref  {tab:SecondExperimentMethods}) and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{29}{figure.caption.19}}
\newlabel{fig:Avg_Accuracy_Spectrum}{{11}{29}{The accuracy spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the accuracy of logistic regression. The legend indicates the algorithms (Table \ref {tab:SecondExperimentMethods}) and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The precision spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the precision of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{30}{figure.caption.20}}
\newlabel{fig:Avg_Precision_Spectrum}{{12}{30}{The precision spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the precision of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The recall spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the recall of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{31}{figure.caption.21}}
\newlabel{fig:Avg_Recall_Spectrum}{{13}{31}{The recall spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the recall of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The F1 spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the F1 score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{32}{figure.caption.22}}
\newlabel{fig:Avg_F1_Spectrum}{{14}{32}{The F1 spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the F1 score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A bar chart showing the average computation time of all evaluated algorithms. The x-axis corresponded to the different algorithms, for which the computation time for multiple parameter combinations were given. The y-axis corresponded to the computation time in seconds. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Support Vector Machine (svm), random forest (rf)\relax }}{34}{figure.caption.23}}
\newlabel{fig:Comp_Time_Bar}{{15}{34}{A bar chart showing the average computation time of all evaluated algorithms. The x-axis corresponded to the different algorithms, for which the computation time for multiple parameter combinations were given. The y-axis corresponded to the computation time in seconds. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The relation between the FS\_accuracy and the computation time. The x-axis shows the computation time the y-axis shows the FS\_accuracy score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{36}{figure.caption.24}}
\newlabel{fig:Comp_Time_FS_Acc}{{16}{36}{The relation between the FS\_accuracy and the computation time. The x-axis shows the computation time the y-axis shows the FS\_accuracy score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}TPOT Feature Selection Integration Results}{37}{subsection.5.3}}
\citation{catal2009investigating}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{liu2002comparative}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{38}{section.6}}
\newlabel{sec:Discussion}{{6}{38}{Discussion}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{38}{section.7}}
\newlabel{sec:Conclusions}{{7}{38}{Conclusions}{section.7}{}}
\bibdata{../References/Citings}
\bibcite{gehlenborg2010visualization}{1}
\bibcite{brazma2001minimum}{2}
\bibcite{cottrell1999probability}{3}
\bibcite{dettmer2007mass}{4}
\bibcite{capitani2017nuclear}{5}
\bibcite{liu2012data}{6}
\bibcite{sittig2008grand}{7}
\bibcite{bertolazzi2008logic}{8}
\bibcite{piatetsky2003microarray}{9}
\bibcite{lommen2009metalign}{10}
\bibcite{holzinger2014knowledge}{11}
\bibcite{wilkins2009proteomics}{12}
\bibcite{teodoro2009biomedical}{13}
\bibcite{doi:10.1093/nar/gkm1037}{14}
\bibcite{sturn2002genesis}{15}
\bibcite{karnovsky2011metscape}{16}
\bibcite{tabas2012genecodis3}{17}
\bibcite{faul2007g}{18}
\bibcite{baumgartner2006data}{19}
\bibcite{welthagen2005comprehensive}{20}
\bibcite{lim2003planar}{21}
\bibcite{peng2010novel}{22}
\bibcite{biesiada2007feature}{23}
\bibcite{ding2005minimum}{24}
\bibcite{catal2009investigating}{25}
\bibcite{liu2002comparative}{26}
\bibcite{nair2009genome}{27}
\bibcite{suarez2012expanding}{28}
\bibcite{bigler2013cross}{29}
\bibcite{yao2008type}{30}
\bibcite{wojnarski2010rsctc}{31}
\bibcite{NIPS2004_2728}{32}
\bibcite{doi:10.1093/bioinformatics/btu022}{33}
\bibcite{OpenML2013}{34}
\bibcite{felix2017dynamic}{35}
\bibcite{madigan2017brock}{36}
\bibcite{Guyon2006}{37}
\bibcite{CATAL20091040}{38}
\bibcite{molina2002feature}{39}
\bibcite{chandrashekar2014survey}{40}
\bibcite{saeys2007review}{41}
\bibcite{Duch2006}{42}
\bibcite{heiberger2004statistical}{43}
\bibcite{jones2014scipy}{44}
\bibcite{peng2005feature}{45}
\bibcite{battiti1994using}{46}
\bibcite{pedregosa2011scikit}{47}
\bibcite{yu2003feature}{48}
\bibcite{hall2000correlation}{49}
\bibcite{donoho2008higher}{50}
\bibcite{storey2003statistical}{51}
\bibcite{higgins2003measuring}{52}
\bibcite{Reunanen2006}{53}
\bibcite{Alsallakh2016PowerSet}{54}
\bibcite{tsamardinos2017massively}{55}
\bibcite{huang2013automated}{56}
\bibcite{zhang2004optimality}{57}
\bibcite{SENAWI201747}{58}
\bibcite{el2009new}{59}
\bibcite{radovic2017minimum}{60}
\bibcite{kirkpatrick1983optimization}{61}
\bibcite{Jirapech-Umpai2005}{62}
\bibcite{Lal2006}{63}
\bibcite{blum1997selection}{64}
\bibcite{jong2004feature}{65}
\bibcite{prados2004mining}{66}
\bibcite{zhang2006recursive}{67}
\bibcite{guyon2002gene}{68}
\bibcite{geurts2005proteomic}{69}
\bibcite{wu2003comparison}{70}
\bibcite{liaw2002classification}{71}
\bibcite{thornton2013auto}{72}
\bibcite{Gijsbers2017Thesis}{73}
\bibcite{kotthoff2016auto}{74}
\bibcite{brazdil1994characterizing}{75}
\bibcite{vilalta2004using}{76}
\bibcite{brazdil2009development}{77}
\bibcite{pazzani1997learning}{78}
\bibcite{chen2006combining}{79}
\bibcite{walt2011numpy}{80}
\bibcite{hall1998practical}{81}
\bibstyle{ieeetr}
\@writefile{toc}{\contentsline {section}{\numberline {A}Feature Selection Exploration Plots}{44}{appendix.A}}
\newlabel{app:FeatureSelectionExplorationPlots}{{A}{44}{Feature Selection Exploration Plots}{appendix.A}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The validation and test score per feature preserved for the micro organisms data setwith ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{44}{figure.caption.27}}
\newlabel{fig:MO_MI_Val_Test_Score}{{17}{44}{The validation and test score per feature preserved for the micro organisms data setwith ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The validation and test score per feature preserved for the micro organisms dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{45}{figure.caption.28}}
\newlabel{fig:MO_T_Val_Test_Score}{{18}{45}{The validation and test score per feature preserved for the micro organisms dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The validation and test score per feature preserved for the Arcene dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{45}{figure.caption.29}}
\newlabel{fig:Arcene_MI_Val_Test_Score}{{19}{45}{The validation and test score per feature preserved for the Arcene dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The validation and test score per feature preserved for the Arcene dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{46}{figure.caption.30}}
\newlabel{fig:Arcene_T_Val_Test_Score}{{20}{46}{The validation and test score per feature preserved for the Arcene dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The validation and test score per feature preserved for the RSCTC dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{46}{figure.caption.31}}
\newlabel{fig:RSCTC_MI_Val_Test_Score}{{21}{46}{The validation and test score per feature preserved for the RSCTC dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The validation and test score per feature preserved for the RSCTC dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{47}{figure.caption.32}}
\newlabel{fig:RSCTC_T_Val_Test_Score}{{22}{47}{The validation and test score per feature preserved for the RSCTC dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces The validation and test score per feature preserved for the Psoriasis dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{47}{figure.caption.33}}
\newlabel{fig:Psoriasis_MI_Val_Test_Score}{{23}{47}{The validation and test score per feature preserved for the Psoriasis dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces The validation and test score per feature preserved for the Psoriasis dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{48}{figure.caption.34}}
\newlabel{fig:Psoriasis_T_Val_Test_Score}{{24}{48}{The validation and test score per feature preserved for the Psoriasis dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Feature Selection Exploration Precision And Recall}{48}{appendix.B}}
\newlabel{app:PrecisionRecall}{{B}{48}{Feature Selection Exploration Precision And Recall}{appendix.B}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces The average precision shown per dataset and rank.\relax }}{48}{figure.caption.35}}
\newlabel{fig:DatasetRankPrecisionScores}{{25}{48}{The average precision shown per dataset and rank.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces The average recall shown per dataset and rank.\relax }}{49}{figure.caption.36}}
\newlabel{fig:DatasetRankRecallScores}{{26}{49}{The average recall shown per dataset and rank.\relax }{figure.caption.36}{}}
