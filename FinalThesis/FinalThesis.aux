\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Abstract}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{gehlenborg2010visualization}
\citation{brazma2001minimum}
\citation{cottrell1999probability}
\citation{dettmer2007mass}
\citation{liu2012data}
\citation{sittig2008grand}
\citation{magni1990chronic}
\citation{gehlenborg2010visualization}
\citation{bertolazzi2008logic}
\citation{piatetsky2003microarray}
\citation{lommen2009metalign}
\citation{holzinger2014knowledge}
\citation{wilkins2009proteomics}
\citation{teodoro2009biomedical}
\citation{doi:10.1093/nar/gkm1037}
\citation{sturn2002genesis}
\citation{karnovsky2011metscape}
\citation{tabas2012genecodis3}
\citation{faul2007g}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{donders2006gentle}
\citation{cartwright2003dealing}
\citation{haukoos2007advanced}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introduction}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Introduction}{{2}{5}{Introduction}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{5}{section.2.1}}
\newlabel{IDsec:Introduction}{{2.1}{5}{Introduction}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Problem Statement}{5}{section.2.2}}
\newlabel{IDsec:ProblemStatement}{{2.2}{5}{Problem Statement}{section.2.2}{}}
\citation{Deneer2017Thesis}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{kim2016spectrum}
\citation{yao2008type}
\citation{suarez2011nonlesional}
\citation{tintle2011reversal}
\citation{gittler2012progressive}
\citation{CIOS20021}
\citation{Yoo2012}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Approach}{6}{section.2.3}}
\newlabel{sec:Approach}{{2.3}{6}{Approach}{section.2.3}{}}
\citation{chen2006medical}
\citation{doi:10.1093/bib/bbx044}
\citation{chen2006medical}
\citation{blythe2008rise}
\citation{Turkay2014}
\citation{Holzinger2014}
\citation{dubitzky2007fundamentals}
\citation{PENG201015}
\citation{dunbar2006spatial}
\citation{devroye1986sample}
\citation{van2002gene}
\citation{roff1989statistical}
\citation{Yoo2012}
\citation{Turkay2014}
\citation{bellazzi2011data}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Preliminaries}{7}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Preliminaries}{{3}{7}{Preliminaries}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Datasets}{7}{section.3.1}}
\newlabel{PLsec:Datasets}{{3.1}{7}{Datasets}{section.3.1}{}}
\citation{Holzinger2014}
\citation{Otasek2014}
\citation{CIOS20021}
\citation{Turkay2014}
\citation{CIOS20021}
\citation{Yoo2012}
\citation{bellazzi2011data}
\citation{Otasek2014}
\citation{marenco2004qis}
\citation{bichutskiy2006heterogeneous}
\citation{sperzel1991biomedical}
\citation{aubry1988design}
\citation{Windridge2014}
\citation{brazma2001minimum}
\citation{selvaraj2011microarray}
\citation{afzal2015fast}
\citation{afzal2015fast}
\citation{selvaraj2011microarray}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{cottrell1999probability}
\citation{dettmer2007mass}
\citation{matthiesen2008analysis}
\citation{watson2007introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Micro-array Datasets}{8}{subsection.3.1.1}}
\newlabel{PLsubsec:Microarray}{{3.1.1}{8}{Micro-array Datasets}{subsection.3.1.1}{}}
\citation{neves2018mass}
\citation{neves2018mass}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{madigan2017brock}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A picture showing the creation of microarray datasets \cite  {afzal2015fast}.\relax }}{9}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:MicroArray}{{3.1}{9}{A picture showing the creation of microarray datasets \cite {afzal2015fast}.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Mass Spectrometry Datasets}{9}{subsection.3.1.2}}
\newlabel{PLsubsec:MassSpect}{{3.1.2}{9}{Mass Spectrometry Datasets}{subsection.3.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Four example figures of the data created by mass spectrometry. The measured molecule masses correspond to the peaks in the figures \cite  {neves2018mass}.\relax }}{10}{figure.caption.3}}
\newlabel{fig:MassSpect}{{3.2}{10}{Four example figures of the data created by mass spectrometry. The measured molecule masses correspond to the peaks in the figures \cite {neves2018mass}.\relax }{figure.caption.3}{}}
\citation{liu2012data}
\citation{sittig2008grand}
\citation{pocock2013clinical}
\citation{bendele1999efficacy}
\citation{pocock2013clinical}
\citation{kan1986short}
\citation{diaconis1983computer}
\citation{cestnikkononenkoj}
\citation{diaconis1983computer}
\citation{cestnikkononenkoj}
\citation{murtaugh1994primary}
\citation{misery2011sensitive}
\citation{fernandes2017transfer}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Clinical Datasets}{11}{subsection.3.1.3}}
\newlabel{PLsubsec:Clinical}{{3.1.3}{11}{Clinical Datasets}{subsection.3.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Survey Datasets}{11}{subsection.3.1.4}}
\newlabel{PLsubsec:Survey}{{3.1.4}{11}{Survey Datasets}{subsection.3.1.4}{}}
\citation{mckinney2010data}
\citation{yan2018hands}
\citation{walt2011NumPy}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces An overview of the symbols used to explain the data\relax }}{12}{table.caption.4}}
\newlabel{tab:DatasetSymbols}{{3.1}{12}{An overview of the symbols used to explain the data\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Data Symbols}{12}{subsection.3.1.5}}
\newlabel{subsec:DataSymbols}{{3.1.5}{12}{Data Symbols}{subsection.3.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Software}{12}{section.3.2}}
\newlabel{PLsec:Software}{{3.2}{12}{Software}{section.3.2}{}}
\citation{jones2014SciPy}
\citation{pedregosa2011scikit}
\citation{mckinney2012Python}
\citation{olson2016tpot}
\citation{yu2003feature}
\citation{hall2000correlation}
\citation{kohavi1995study}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Concepts}{13}{section.3.3}}
\newlabel{PLsec:Concepts}{{3.3}{13}{Concepts}{section.3.3}{}}
\newlabel{eq:Correlation}{{3.1}{13}{Concepts}{equation.3.3.1}{}}
\citation{kearns1999algorithmic}
\citation{agresti2003categorical}
\citation{edwards2002explaining}
\citation{han2011data}
\citation{yu2003feature}
\citation{lim2003planar}
\citation{biesiada2007feature}
\citation{peng2005feature}
\citation{battiti1994using}
\newlabel{eq:MutualInformation}{{3.2}{15}{Concepts}{equation.3.3.2}{}}
\citation{blackman2000interval}
\newlabel{eq:Accuracy}{{3.3}{16}{Concepts}{equation.3.3.3}{}}
\newlabel{eq:Precision}{{3.4}{16}{Concepts}{equation.3.3.4}{}}
\newlabel{eq:Recall}{{3.5}{16}{Concepts}{equation.3.3.5}{}}
\newlabel{eq:F1}{{3.6}{16}{Concepts}{equation.3.3.6}{}}
\newlabel{eq:Kappa}{{3.7}{16}{Concepts}{equation.3.3.7}{}}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{lim2003planar}
\citation{peng2010novel}
\citation{biesiada2007feature}
\citation{ding2005minimum}
\citation{catal2009investigating}
\citation{liu2002comparative}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Feature Selection}{17}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:FeatureSelection}{{4}{17}{Feature Selection}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{17}{section.4.1}}
\newlabel{FSsec:Introduction}{{4.1}{17}{Introduction}{section.4.1}{}}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{yao2008type}
\citation{wojnarski2010rsctc}
\citation{NIPS2004_2728}
\citation{doi:10.1093/bioinformatics/btu022}
\citation{Guyon2006}
\citation{CATAL20091040}
\citation{molina2002feature}
\citation{chandrasheto split bykar2014survey}
\citation{saeys2007review}
\citation{Duch2006}
\citation{saeys2007review}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces A schematic overview of the four datasets.\relax }}{18}{table.caption.5}}
\newlabel{tab:DataSetDescriptions}{{4.1}{18}{A schematic overview of the four datasets.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Background}{18}{section.4.2}}
\newlabel{FSsec:Background}{{4.2}{18}{Background}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Datasets}{18}{subsection.4.2.1}}
\newlabel{FSsubsec:Datasets}{{4.2.1}{18}{Datasets}{subsection.4.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Feature Selection Methods}{18}{subsection.4.2.2}}
\newlabel{FSsubsec:FeatureSelectionMethods}{{4.2.2}{18}{Feature Selection Methods}{subsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Filter Methods}{18}{section*.6}}
\newlabel{FSsubsec:FilterMethods}{{4.2.2}{18}{Filter Methods}{section*.6}{}}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{donoho2008higher}
\citation{storey2003statistical}
\citation{higgins2003measuring}
\citation{Duch2006}
\citation{Duch2006}
\citation{Duch2006}
\citation{Duch2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces A basic top $n$ filter algorithm \cite  {Duch2006}\relax }}{19}{algorithm.1}}
\newlabel{alg:FilterTopNAlgorithm}{{1}{19}{A basic top $n$ filter algorithm \cite {Duch2006}\relax }{algorithm.1}{}}
\citation{Duch2006}
\citation{saeys2007review}
\citation{Reunanen2006}
\citation{Alsallakh2016PowerSet}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{tsamardinos2017massively}
\citation{huang2013automated}
\citation{saeys2007review}
\citation{SENAWI201747}
\citation{el2009new}
\citation{radovic2017minimum}
\citation{Reunanen2006}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces A basic filter algorithm \cite  {Duch2006}\relax }}{20}{algorithm.2}}
\newlabel{alg:FilterAlgorithm}{{2}{20}{A basic filter algorithm \cite {Duch2006}\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Wrapper Methods}{20}{section*.7}}
\newlabel{FSsubsec:WrapperMethods}{{4.2.2}{20}{Wrapper Methods}{section*.7}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces A forward selection sequential search algorithm \cite  {Reunanen2006}\relax }}{21}{algorithm.3}}
\newlabel{alg:ForwardSelection}{{3}{21}{A forward selection sequential search algorithm \cite {Reunanen2006}\relax }{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces A backward selection sequential search algorithm \cite  {Reunanen2006}\relax }}{21}{algorithm.4}}
\newlabel{alg:BackwardSelection}{{4}{21}{A backward selection sequential search algorithm \cite {Reunanen2006}\relax }{algorithm.4}{}}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{Reunanen2006}
\citation{kirkpatrick1983optimization}
\citation{Reunanen2006}
\citation{Jirapech-Umpai2005}
\citation{Reunanen2006}
\citation{Reunanen2006}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces A plus l-take away r sequential search algorithm \cite  {Reunanen2006}\relax }}{22}{algorithm.5}}
\newlabel{alg:PTA}{{5}{22}{A plus l-take away r sequential search algorithm \cite {Reunanen2006}\relax }{algorithm.5}{}}
\citation{Reunanen2006}
\citation{saeys2007review}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces A floating search algorithm \cite  {Reunanen2006}\relax }}{23}{algorithm.6}}
\newlabel{alg:FloatingSearch}{{6}{23}{A floating search algorithm \cite {Reunanen2006}\relax }{algorithm.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Simulated Annealing search algorithm \cite  {Reunanen2006}\relax }}{23}{algorithm.7}}
\newlabel{alg:SASearch}{{7}{23}{Simulated Annealing search algorithm \cite {Reunanen2006}\relax }{algorithm.7}{}}
\citation{Lal2006}
\citation{blum1997selection}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\@writefile{toc}{\contentsline {subsubsection}{Embedded Methods}{24}{section*.8}}
\newlabel{FSsubsec:EmbeddedMethods}{{4.2.2}{24}{Embedded Methods}{section*.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces An embedded forward selection algorithm \cite  {Lal2006}\relax }}{24}{algorithm.8}}
\newlabel{alg:EmbeddedForwardSelectionAlgorithm}{{8}{24}{An embedded forward selection algorithm \cite {Lal2006}\relax }{algorithm.8}{}}
\citation{Lal2006}
\citation{Lal2006}
\citation{Lal2006}
\citation{jong2004feature}
\citation{prados2004mining}
\citation{zhang2006recursive}
\citation{guyon2002gene}
\citation{geurts2005proteomic}
\citation{wu2003comparison}
\citation{Duch2006}
\citation{liaw2002classification}
\citation{thornton2013auto}
\citation{Gijsbers2017Thesis}
\citation{thornton2013auto}
\citation{kotthoff2016auto}
\citation{Gijsbers2017Thesis}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces An embedded backward elimination algorithm \cite  {Lal2006} \relax }}{25}{algorithm.9}}
\newlabel{alg:EmbeddedBackwardEliminationAlgorithm}{{9}{25}{An embedded backward elimination algorithm \cite {Lal2006} \relax }{algorithm.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Automated Machine Learning}{25}{subsection.4.2.3}}
\newlabel{FSsubsec:AutomatedMachineLearning}{{4.2.3}{25}{Automated Machine Learning}{subsection.4.2.3}{}}
\citation{koza1997genetic}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\citation{brazdil1994characterizing}
\citation{vilalta2004using}
\citation{brazdil2009development}
\@writefile{toc}{\contentsline {subsubsection}{Meta-Learning}{26}{section*.9}}
\newlabel{FSsubsec:Meta-Learning}{{4.2.3}{26}{Meta-Learning}{section*.9}{}}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\citation{Gijsbers2017Thesis}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A layout of how meta-learning works. 1. Data sets are collected. 2. Meta-data is computed for each dataset. 3. A meta-dataset is created and a meta-model is learned \cite  {Gijsbers2017Thesis}.\relax }}{27}{figure.caption.10}}
\newlabel{fig:Meta-LearningLayout}{{4.1}{27}{A layout of how meta-learning works. 1. Data sets are collected. 2. Meta-data is computed for each dataset. 3. A meta-dataset is created and a meta-model is learned \cite {Gijsbers2017Thesis}.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tree-based Pipeline Optimization Tool}{27}{section*.11}}
\newlabel{FSsubsec:TPOT}{{4.2.3}{27}{Tree-based Pipeline Optimization Tool}{section*.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces An example of a machine learning pipeline in \textit  {TPOT}. It only shows the primitive algorithms and not hyper parameter terminals. At the root is the machine learning algorithm \cite  {Gijsbers2017Thesis}.\relax }}{28}{figure.caption.12}}
\newlabel{fig:MachineLearningPipeline}{{4.2}{28}{An example of a machine learning pipeline in \textit {TPOT}. It only shows the primitive algorithms and not hyper parameter terminals. At the root is the machine learning algorithm \cite {Gijsbers2017Thesis}.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces All \textit  {TPOT} Algorithms used for the \textit  {TPOT} classifier function, their \textit  {scikit-learn} class name is given. Not only the algorithms differ, but also the hyper-parameters within these algorithms.\relax }}{28}{table.caption.13}}
\newlabel{tab:TPOTAlgorithms}{{4.2}{28}{All \textit {TPOT} Algorithms used for the \textit {TPOT} classifier function, their \textit {scikit-learn} class name is given. Not only the algorithms differ, but also the hyper-parameters within these algorithms.\relax }{table.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Examples of the three mutations in the \textit  {TPOT} algorithm: insertion, replacement and shrinking \cite  {Gijsbers2017Thesis}.\relax }}{29}{figure.caption.14}}
\newlabel{fig:TPOTMutations}{{4.3}{29}{Examples of the three mutations in the \textit {TPOT} algorithm: insertion, replacement and shrinking \cite {Gijsbers2017Thesis}.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces An example of a \textit  {TPOT} crossover \cite  {Gijsbers2017Thesis}.\relax }}{29}{figure.caption.15}}
\newlabel{fig:TPOTCrossover}{{4.4}{29}{An example of a \textit {TPOT} crossover \cite {Gijsbers2017Thesis}.\relax }{figure.caption.15}{}}
\citation{pazzani1997learning}
\citation{chen2006combining}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Hypotheses}{30}{section.4.3}}
\newlabel{FSsec:hypotheses}{{4.3}{30}{Hypotheses}{section.4.3}{}}
\citation{hall1998practical}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Methods}{31}{section.4.4}}
\newlabel{FSsec:Methods}{{4.4}{31}{Methods}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Feature Selection Quality}{31}{subsection.4.4.1}}
\newlabel{FSsubsec:DimensionalityReductionQuality}{{4.4.1}{31}{Feature Selection Quality}{subsection.4.4.1}{}}
\newlabel{eq:FSAccuracy}{{4.4.1}{31}{Feature Selection Quality}{subsection.4.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The correction factor for the \textit  {FS\_score} from Equation \ref  {eq:FSAccuracy}. Every graph corresponds to a different value of $\beta $.\relax }}{32}{figure.caption.16}}
\newlabel{fig:FSAccuracyPlot}{{4.5}{32}{The correction factor for the \textit {FS\_score} from Equation \ref {eq:FSAccuracy}. Every graph corresponds to a different value of $\beta $.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces An example of the impact of the correction factor on the score, in this case accuracy. The shown correction factor uses $\beta = 0.99$. On the x-axis the number of features is shown and on the y-axis the value for the original accuracy, the correction factor and the \textit  {FS\_score}.\relax }}{33}{figure.caption.17}}
\newlabel{fig:FSAccuracyExample}{{4.6}{33}{An example of the impact of the correction factor on the score, in this case accuracy. The shown correction factor uses $\beta = 0.99$. On the x-axis the number of features is shown and on the y-axis the value for the original accuracy, the correction factor and the \textit {FS\_score}.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Feature Selection Exploration}{33}{subsection.4.4.2}}
\newlabel{FSsubsec:FeatureSelectionExploration}{{4.4.2}{33}{Feature Selection Exploration}{subsection.4.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Feature Selection Algorithms Evaluation}{33}{subsection.4.4.3}}
\newlabel{FSsubsec:FeatureSelectionAlgorithmsEvaluation}{{4.4.3}{33}{Feature Selection Algorithms Evaluation}{subsection.4.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The four meta-parameters with their possible values in the first experiment.\relax }}{34}{table.caption.18}}
\newlabel{tab:FirstExperimentRequirements}{{4.3}{34}{The four meta-parameters with their possible values in the first experiment.\relax }{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces The methods that are evaluated in the second experiment setup.\relax }}{35}{table.caption.19}}
\newlabel{tab:SecondExperimentMethods}{{4.4}{35}{The methods that are evaluated in the second experiment setup.\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}TPOT Feature selection integration}{35}{subsection.4.4.4}}
\newlabel{FSsubsec:TPOTEvaluationIntegration}{{4.4.4}{35}{TPOT Feature selection integration}{subsection.4.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces The experiment details for testing the non-trivial changes in \textit  {TPOT}. This experiment is rerun 5 times.\relax }}{37}{table.caption.20}}
\newlabel{tab:TPOTExpDetails}{{4.5}{37}{The experiment details for testing the non-trivial changes in \textit {TPOT}. This experiment is rerun 5 times.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Results}{38}{section.4.5}}
\newlabel{FSsec:Results}{{4.5}{38}{Results}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Feature Selection Exploration Results}{38}{subsection.4.5.1}}
\newlabel{FSsubsec:FeatureReductionExplorationResults}{{4.5.1}{38}{Feature Selection Exploration Results}{subsection.4.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The average validation and test scores after averaging the scores for the data sets and ranking methods.\relax }}{38}{figure.caption.21}}
\newlabel{fig:ValTestScores}{{4.7}{38}{The average validation and test scores after averaging the scores for the data sets and ranking methods.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The average validation accuracy shown per machine learning quality measurement.\relax }}{39}{figure.caption.22}}
\newlabel{fig:MachineLearningQualityScores}{{4.8}{39}{The average validation accuracy shown per machine learning quality measurement.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces The average validation accuracy shown per dataset and rank.\relax }}{39}{figure.caption.23}}
\newlabel{fig:DatasetRankScores}{{4.9}{39}{The average validation accuracy shown per dataset and rank.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces The average validation F1-scores shown per dataset and rank.\relax }}{40}{figure.caption.24}}
\newlabel{fig:DatasetRankF1Scores}{{4.10}{40}{The average validation F1-scores shown per dataset and rank.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Feature Selection Algorithms Evaluation Results}{40}{subsection.4.5.2}}
\newlabel{FSsubsec:FeatureSelectionAlgorithmsEvaluationResults}{{4.5.2}{40}{Feature Selection Algorithms Evaluation Results}{subsection.4.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The accuracy spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the accuracy of logistic regression. The legend indicates the algorithms (Table \ref  {tab:SecondExperimentMethods}) and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{41}{figure.caption.25}}
\newlabel{fig:Avg_Accuracy_Spectrum}{{4.11}{41}{The accuracy spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the accuracy of logistic regression. The legend indicates the algorithms (Table \ref {tab:SecondExperimentMethods}) and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces The F1 spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the F1 score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{42}{figure.caption.26}}
\newlabel{fig:Avg_F1_Spectrum}{{4.12}{42}{The F1 spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the F1 score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces A bar chart showing the average computation time of all evaluated algorithms. The x-axis corresponded to the different algorithms, for which the computation time for multiple parameter combinations were given. The y-axis corresponded to the computation time in seconds. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Support Vector Machine (svm), random forest (rf)\relax }}{44}{figure.caption.27}}
\newlabel{fig:Comp_Time_Bar}{{4.13}{44}{A bar chart showing the average computation time of all evaluated algorithms. The x-axis corresponded to the different algorithms, for which the computation time for multiple parameter combinations were given. The y-axis corresponded to the computation time in seconds. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces The relation between the \textit  {FS\_score} and the computation time. The x-axis shows the computation time the y-axis shows the \textit  {FS\_score} score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{46}{figure.caption.28}}
\newlabel{fig:Comp_Time_FS_Acc}{{4.14}{46}{The relation between the \textit {FS\_score} and the computation time. The x-axis shows the computation time the y-axis shows the \textit {FS\_score} score of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces The performance of the final pipeline for the different types of \textit  {TPOT} and the mentioned dataset. The five reruns are averaged into this one result.\relax }}{47}{table.caption.29}}
\newlabel{tab:TPOTResults}{{4.6}{47}{The performance of the final pipeline for the different types of \textit {TPOT} and the mentioned dataset. The five reruns are averaged into this one result.\relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}TPOT Feature Selection Integration Results}{47}{subsection.4.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces The optimization process for the different \textit  {TPOT} algorithms for the micro-organisms dataset.\relax }}{47}{figure.caption.30}}
\newlabel{fig:TPOTResultMO}{{4.15}{47}{The optimization process for the different \textit {TPOT} algorithms for the micro-organisms dataset.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces The optimization process for the different \textit  {TPOT} algorithms for the Arcene dataset.\relax }}{48}{figure.caption.31}}
\newlabel{fig:TPOTResultArcene}{{4.16}{48}{The optimization process for the different \textit {TPOT} algorithms for the Arcene dataset.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces The optimization process for the different \textit  {TPOT} algorithms for the RSCTC dataset.\relax }}{48}{figure.caption.32}}
\newlabel{fig:TPOTResultRSCTC}{{4.17}{48}{The optimization process for the different \textit {TPOT} algorithms for the RSCTC dataset.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces The optimization process for the different \textit  {TPOT} algorithms for the psoriasis dataset.\relax }}{49}{figure.caption.33}}
\newlabel{fig:TPOTResultPsoriasis}{{4.18}{49}{The optimization process for the different \textit {TPOT} algorithms for the psoriasis dataset.\relax }{figure.caption.33}{}}
\citation{catal2009investigating}
\citation{baumgartner2006data}
\citation{welthagen2005comprehensive}
\citation{liu2002comparative}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Discussion}{50}{section.4.6}}
\newlabel{FSsec:Discussion}{{4.6}{50}{Discussion}{section.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Conclusions}{50}{section.4.7}}
\newlabel{FSsec:Conclusions}{{4.7}{50}{Conclusions}{section.4.7}{}}
\citation{donders2006gentle}
\citation{cartwright2003dealing}
\citation{haukoos2007advanced}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Missing Value Handling}{52}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:MissingValueHandling}{{5}{52}{Missing Value Handling}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{52}{section.5.1}}
\newlabel{MVsec:Introduction}{{5.1}{52}{Introduction}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Background}{52}{section.5.2}}
\newlabel{sec:Background}{{5.2}{52}{Background}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Datasets}{52}{subsection.5.2.1}}
\newlabel{MVsubsec:Datasets}{{5.2.1}{52}{Datasets}{subsection.5.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces A schematic overview of the four datasets\relax }}{52}{table.caption.34}}
\newlabel{tab:Datasets}{{5.1}{52}{A schematic overview of the four datasets\relax }{table.caption.34}{}}
\citation{patrician2002multiple}
\citation{donders2006gentle}
\citation{cartwright2003dealing}
\citation{haukoos2007advanced}
\citation{donders2006gentle}
\citation{haukoos2007advanced}
\citation{cartwright2003dealing}
\citation{donders2006gentle}
\citation{cartwright2003dealing}
\citation{haukoos2007advanced}
\citation{haukoos2007advanced}
\citation{donders2006gentle}
\citation{haukoos2007advanced}
\citation{donders2006gentle}
\citation{cartwright2003dealing}
\citation{haukoos2007advanced}
\citation{donders2006gentle}
\citation{patrician2002multiple}
\citation{sterne2009multiple}
\citation{myrtveit2001analyzing}
\citation{haukoos2007advanced}
\citation{patrician2002multiple}
\citation{myrtveit2001analyzing}
\citation{donders2006gentle}
\citation{cartwright2003dealing}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Missing Value Handling}{53}{subsection.5.2.2}}
\newlabel{MVsubsec:MissingValueHandling}{{5.2.2}{53}{Missing Value Handling}{subsection.5.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{List Deletion}{53}{section*.35}}
\newlabel{MVsubsec:ListDeletion}{{5.2.2}{53}{List Deletion}{section*.35}{}}
\citation{haukoos2007advanced}
\citation{donders2006gentle}
\citation{patrician2002multiple}
\citation{myrtveit2001analyzing}
\citation{haukoos2007advanced}
\citation{donders2006gentle}
\citation{patrician2002multiple}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces Complete Case Analysis\relax }}{54}{algorithm.10}}
\newlabel{alg:CCA}{{10}{54}{Complete Case Analysis\relax }{algorithm.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces Available Case Analysis\relax }}{54}{algorithm.11}}
\newlabel{alg:ACA}{{11}{54}{Available Case Analysis\relax }{algorithm.11}{}}
\citation{cartwright2003dealing}
\citation{haukoos2007advanced}
\citation{myrtveit2001analyzing}
\citation{donders2006gentle}
\citation{haukoos2007advanced}
\citation{donders2006gentle}
\citation{myrtveit2001analyzing}
\citation{pedersen2017missing}
\citation{donders2006gentle}
\@writefile{loa}{\contentsline {algorithm}{\numberline {12}{\ignorespaces Weighted Case Analysis\relax }}{55}{algorithm.12}}
\newlabel{alg:WCA}{{12}{55}{Weighted Case Analysis\relax }{algorithm.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Single Imputation}{55}{section*.36}}
\newlabel{MVsubsec:SingleImputation}{{5.2.2}{55}{Single Imputation}{section*.36}{}}
\citation{haukoos2007advanced}
\citation{myrtveit2001analyzing}
\citation{cartwright2003dealing}
\citation{donders2006gentle}
\citation{pedersen2017missing}
\citation{myrtveit2001analyzing}
\citation{cartwright2003dealing}
\citation{haukoos2007advanced}
\citation{haukoos2007advanced}
\@writefile{loa}{\contentsline {algorithm}{\numberline {13}{\ignorespaces Missing Indicator Imputation\relax }}{56}{algorithm.13}}
\newlabel{alg:MissingIndicatorImputation}{{13}{56}{Missing Indicator Imputation\relax }{algorithm.13}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {14}{\ignorespaces Mean Imputation\relax }}{56}{algorithm.14}}
\newlabel{alg:MeanImputation}{{14}{56}{Mean Imputation\relax }{algorithm.14}{}}
\citation{raghunathan2001multivariate}
\citation{donders2006gentle}
\citation{myrtveit2001analyzing}
\citation{cartwright2003dealing}
\citation{donders2006gentle}
\@writefile{loa}{\contentsline {algorithm}{\numberline {15}{\ignorespaces Hot Deck Imputation\relax }}{57}{algorithm.15}}
\newlabel{alg:HotDeckImputation}{{15}{57}{Hot Deck Imputation\relax }{algorithm.15}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {16}{\ignorespaces Multivariate Regression Imputation\relax }}{57}{algorithm.16}}
\newlabel{alg:RegressionImputation}{{16}{57}{Multivariate Regression Imputation\relax }{algorithm.16}{}}
\citation{pedersen2017missing}
\citation{haukoos2007advanced}
\citation{haukoos2007advanced}
\citation{cartwright2003dealing}
\citation{donders2006gentle}
\@writefile{loa}{\contentsline {algorithm}{\numberline {17}{\ignorespaces k Nearest Neighbour Imputation\relax }}{58}{algorithm.17}}
\newlabel{alg:kNNImputation}{{17}{58}{k Nearest Neighbour Imputation\relax }{algorithm.17}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {18}{\ignorespaces k Nearest Neighbour Imputation\relax }}{58}{algorithm.18}}
\newlabel{alg:WorstCaseImputation}{{18}{58}{k Nearest Neighbour Imputation\relax }{algorithm.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multiple Imputation}{58}{section*.37}}
\newlabel{DEsubsec:MultipleImputation}{{5.2.2}{58}{Multiple Imputation}{section*.37}{}}
\citation{chevret2015multiple}
\citation{rubin1976inference}
\citation{chevret2015multiple}
\citation{rubin1976inference}
\citation{pedersen2017missing}
\citation{white2011multiple}
\citation{donders2006gentle}
\citation{he2010multiple}
\citation{van2007multiple}
\citation{pedersen2017missing}
\citation{van1999multiple}
\citation{van2006imputation}
\citation{azur2011multiple}
\citation{royston2004multiple}
\citation{martin2018impact}
\citation{raghunathan2001multivariate}
\citation{white2011multiple}
\citation{donders2006gentle}
\citation{van2006imputation}
\citation{van2006imputation}
\citation{pedersen2017missing}
\citation{pedersen2017missing}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An graphical layout of multiple imputation. At the start a dataset with missing values is present. Missing values are imputed $m$ times to create $m$ imputed datasets. The $m$ complete datasets are all analysed for $m$ different sets of estimates. These results are then pooled into one complete set of estimates. \cite  {chevret2015multiple, rubin1976inference}\relax }}{59}{figure.caption.38}}
\newlabel{fig:MultipleImputationLayout}{{5.1}{59}{An graphical layout of multiple imputation. At the start a dataset with missing values is present. Missing values are imputed $m$ times to create $m$ imputed datasets. The $m$ complete datasets are all analysed for $m$ different sets of estimates. These results are then pooled into one complete set of estimates. \cite {chevret2015multiple, rubin1976inference}\relax }{figure.caption.38}{}}
\citation{horton2001multiple}
\citation{allison2000multiple}
\citation{royston2011multiple}
\citation{azur2011multiple}
\citation{royston2004multiple}
\citation{he2010multiple}
\citation{azur2011multiple}
\citation{azur2011multiple}
\citation{royston2004multiple}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Hypotheses}{60}{section.5.3}}
\newlabel{MVsec:Hypothesis}{{5.3}{60}{Hypotheses}{section.5.3}{}}
\citation{heiberger2004statistical}
\citation{bartlett1935effect}
\citation{brown1974robust}
\citation{satorra2001scaled}
\citation{satorra2001scaled}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Methods}{61}{section.5.4}}
\newlabel{MVsec:Methods}{{5.4}{61}{Methods}{section.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Bias Evaluation}{61}{subsection.5.4.1}}
\newlabel{MVsubsec:BiasEvaluation}{{5.4.1}{61}{Bias Evaluation}{subsection.5.4.1}{}}
\citation{draper2014applied}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces The three different aspects of the bias evaluation. Four different datasets are used. Two tests are done per feature type, to test hypothesis Hyp0 and seven missing value handling algorithms are implemented to be tested.\relax }}{62}{table.caption.39}}
\newlabel{tab:BiasEvaluationTable}{{5.2}{62}{The three different aspects of the bias evaluation. Four different datasets are used. Two tests are done per feature type, to test hypothesis Hyp0 and seven missing value handling algorithms are implemented to be tested.\relax }{table.caption.39}{}}
\newlabel{eq:R2}{{5.1}{62}{Bias Evaluation}{equation.5.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Quality Evaluation}{63}{subsection.5.4.2}}
\newlabel{MVsubsec:QualityEvaluation}{{5.4.2}{63}{Quality Evaluation}{subsection.5.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces All tested missing value handling methods and possible parameters used during testing\relax }}{63}{table.caption.40}}
\newlabel{tab:MethodsExplanation}{{5.3}{63}{All tested missing value handling methods and possible parameters used during testing\relax }{table.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Results}{63}{section.5.5}}
\newlabel{MVsec:Results}{{5.5}{63}{Results}{section.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Bias Evaluation Results}{63}{subsection.5.5.1}}
\newlabel{MVsubsec:BiasEvaluationResults}{{5.5.1}{63}{Bias Evaluation Results}{subsection.5.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Testing for the heart attack data set whether the type of missing values can be represented by the remaining values. This is done by comparing distributions between the old and new data after CCA and WCA. For numeric values, two tests were used, an independent t-test for equality of mean and a Levene's test with the median in brackets to test equality of variance. For ordinal and categorical features the medians and modes where compared respectively as well as a chi squared test in brackets for equality of distribution. P-values lower than $p < 0.05$ are marked red for failure of representation, p-values higher than $p > 0.05$ are marked green for correctly being represented. If at least one feature is not represented after CCA, the missing values cannot be MCAR. If at least one feature is not represented after WCA, the pseudo-randomness cannot be corrected by only using weights for other values.\relax }}{64}{table.caption.41}}
\newlabel{tab:LDHeartAttack}{{5.4}{64}{Testing for the heart attack data set whether the type of missing values can be represented by the remaining values. This is done by comparing distributions between the old and new data after CCA and WCA. For numeric values, two tests were used, an independent t-test for equality of mean and a Levene's test with the median in brackets to test equality of variance. For ordinal and categorical features the medians and modes where compared respectively as well as a chi squared test in brackets for equality of distribution. P-values lower than $p < 0.05$ are marked red for failure of representation, p-values higher than $p > 0.05$ are marked green for correctly being represented. If at least one feature is not represented after CCA, the missing values cannot be MCAR. If at least one feature is not represented after WCA, the pseudo-randomness cannot be corrected by only using weights for other values.\relax }{table.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces A table showing the number of samples that remained after performing CCA.\relax }}{65}{table.caption.42}}
\newlabel{tab:CCARemainingSamples}{{5.5}{65}{A table showing the number of samples that remained after performing CCA.\relax }{table.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Plots showing the probability of bias being present in feature distributions. The mean of the features with the old distribution with missing values and the new distribution after missing value handling are compared and the p-value of them both originating from the same distribution is computed. Every feature of every dataset is a data point. On the x-axis the percentage of missing values is given for a feature and on the y-axis the p-value of the probability. For every missing value algorithm also a linear fit is made to show the trend of the scatter plot.\relax }}{66}{figure.caption.44}}
\newlabel{fig:PMeanFits}{{5.2}{66}{Plots showing the probability of bias being present in feature distributions. The mean of the features with the old distribution with missing values and the new distribution after missing value handling are compared and the p-value of them both originating from the same distribution is computed. Every feature of every dataset is a data point. On the x-axis the percentage of missing values is given for a feature and on the y-axis the p-value of the probability. For every missing value algorithm also a linear fit is made to show the trend of the scatter plot.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Plots showing the probability of bias being present in feature distributions. The variance of the features with the old distribution with missing values and the new distribution after missing value handling are compared and the p-value of them both originating from the same distribution is computed. Every feature of every dataset is a data point. On the x-axis the percentage of missing values is given for a feature and on the y-axis the p-value of the probability. For every missing value algorithm also a linear fit is made to show the trend of the scatter plot.\relax }}{67}{figure.caption.45}}
\newlabel{fig:PVarianceFits}{{5.3}{67}{Plots showing the probability of bias being present in feature distributions. The variance of the features with the old distribution with missing values and the new distribution after missing value handling are compared and the p-value of them both originating from the same distribution is computed. Every feature of every dataset is a data point. On the x-axis the percentage of missing values is given for a feature and on the y-axis the p-value of the probability. For every missing value algorithm also a linear fit is made to show the trend of the scatter plot.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Plots showing the probability of bias being present in feature distributions. The mean of the features with the old distribution with missing values and the new distribution after missing value handling are compared and the p-value of them both originating from the same distribution is computed. Every feature of every dataset is a data point, however features with more than 15\% missing values are removed. On the x-axis the percentage of missing values is given for a feature and on the y-axis the p-value of the probability. For every missing value algorithm also a linear fit is made to show the trend of the scatter plot.\relax }}{68}{figure.caption.46}}
\newlabel{fig:PMeanFitsACA}{{5.4}{68}{Plots showing the probability of bias being present in feature distributions. The mean of the features with the old distribution with missing values and the new distribution after missing value handling are compared and the p-value of them both originating from the same distribution is computed. Every feature of every dataset is a data point, however features with more than 15\% missing values are removed. On the x-axis the percentage of missing values is given for a feature and on the y-axis the p-value of the probability. For every missing value algorithm also a linear fit is made to show the trend of the scatter plot.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Plots showing the probability of bias being present in feature distributions. The variance of the features with the old distribution with missing values and the new distribution after missing value handling are compared and the p-value of them both originating from the same distribution is computed. Every feature of every dataset is a data point, however features with more than 15\% missing values are removed. On the x-axis the percentage of missing values is given for a feature and on the y-axis the p-value of the probability. For every missing value algorithm also a linear fit is made to show the trend of the scatter plot.\relax }}{69}{figure.caption.47}}
\newlabel{fig:PVarianceFitsACA}{{5.5}{69}{Plots showing the probability of bias being present in feature distributions. The variance of the features with the old distribution with missing values and the new distribution after missing value handling are compared and the p-value of them both originating from the same distribution is computed. Every feature of every dataset is a data point, however features with more than 15\% missing values are removed. On the x-axis the percentage of missing values is given for a feature and on the y-axis the p-value of the probability. For every missing value algorithm also a linear fit is made to show the trend of the scatter plot.\relax }{figure.caption.47}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Testing for the heart attack data set if certain types of imputation create a vastly different distribution for features with missing values. The imputation values are generated with mean imputation, hot deck imputation, k-Nearest Neighbour imputation ($k = 3$), regression imputation and MICE (number of cycles is $s = 5$). For numeric values, two tests were used, an independent t-test for equality of mean and a Levene's test with the median in brackets to test equality of variance. For ordinal and categorical features the medians and modes where checked respectively to be similar as well as a chi squared test in brackets fro equality of distribution. P-values lower than $p < 0.05$ are marked red for failure of representation, p-values close to $p > 0.05$ are marked green for correctly being represented.\relax }}{70}{table.caption.43}}
\newlabel{tab:ImputationHeartAttack}{{5.6}{70}{Testing for the heart attack data set if certain types of imputation create a vastly different distribution for features with missing values. The imputation values are generated with mean imputation, hot deck imputation, k-Nearest Neighbour imputation ($k = 3$), regression imputation and MICE (number of cycles is $s = 5$). For numeric values, two tests were used, an independent t-test for equality of mean and a Levene's test with the median in brackets to test equality of variance. For ordinal and categorical features the medians and modes where checked respectively to be similar as well as a chi squared test in brackets fro equality of distribution. P-values lower than $p < 0.05$ are marked red for failure of representation, p-values close to $p > 0.05$ are marked green for correctly being represented.\relax }{table.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Quality Evaluation}{70}{subsection.5.5.2}}
\newlabel{MVsubsec:QualityEvaluationResults}{{5.5.2}{70}{Quality Evaluation}{subsection.5.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces A plot showing the average accuracy for the datasets classified after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }}{71}{figure.caption.48}}
\newlabel{fig:EvalAvgAcc}{{5.6}{71}{A plot showing the average accuracy for the datasets classified after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces A plot showing the average accuracy for the datasets classified after using a missing value handling algorithm and after removing all features with more than 15\% missing values. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }}{72}{figure.caption.49}}
\newlabel{fig:EvalAvgAccExtraACA}{{5.7}{72}{A plot showing the average accuracy for the datasets classified after using a missing value handling algorithm and after removing all features with more than 15\% missing values. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces A plot showing the F1-score for the datasets classified after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }}{73}{figure.caption.50}}
\newlabel{fig:EvalAvgF1}{{5.8}{73}{A plot showing the F1-score for the datasets classified after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }{figure.caption.50}{}}
\citation{donders2006gentle}
\citation{haukoos2007advanced}
\citation{patrician2002multiple}
\citation{sterne2009multiple}
\citation{pedersen2017missing}
\citation{raghunathan2001multivariate}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces The classification results of the datasets and the average after removing all features with missing values, as well as the best results after handling missing values.\relax }}{74}{table.caption.51}}
\newlabel{tab:ClassResults}{{5.7}{74}{The classification results of the datasets and the average after removing all features with missing values, as well as the best results after handling missing values.\relax }{table.caption.51}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Discussion}{75}{section.5.6}}
\newlabel{MVsec:Discussion}{{5.6}{75}{Discussion}{section.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Conclusions}{75}{section.5.7}}
\newlabel{MVsec:Conclusions}{{5.7}{75}{Conclusions}{section.5.7}{}}
\citation{kluegl2009meta}
\citation{diaconis1983computer}
\citation{cestnikkononenkoj}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Dataset Exploration}{76}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:DatasetExploration}{{6}{76}{Dataset Exploration}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{76}{section.6.1}}
\newlabel{DEsec:Introduction}{{6.1}{76}{Introduction}{section.6.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Background}{76}{section.6.2}}
\newlabel{DEsec:Background}{{6.2}{76}{Background}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Datasets}{76}{subsection.6.2.1}}
\newlabel{DEsubsec:Datasets}{{6.2.1}{76}{Datasets}{subsection.6.2.1}{}}
\citation{congdon2005bayesian}
\citation{safavian1991survey}
\citation{gelman2006data}
\citation{guo2016entity}
\citation{larose2014discovering}
\citation{al2006normalization}
\citation{patro2015normalization}
\citation{al2006normalization}
\citation{patro2015normalization}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Preprocessing and Analysis}{77}{subsection.6.2.2}}
\newlabel{DEsubsec:Preprocessing}{{6.2.2}{77}{Preprocessing and Analysis}{subsection.6.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Feature Types}{77}{section*.52}}
\newlabel{DEsubsec:FeatureTypes}{{6.2.2}{77}{Feature Types}{section*.52}{}}
\@writefile{toc}{\contentsline {subsubsection}{Feature Importance Imbalance}{77}{section*.53}}
\newlabel{DEsubsec:Normalisation}{{6.2.2}{77}{Feature Importance Imbalance}{section*.53}{}}
\citation{agresti2003categorical}
\citation{peng2005feature}
\@writefile{toc}{\contentsline {subsubsection}{Output Imbalance}{78}{section*.54}}
\newlabel{DEsubsec:OutputImbalance}{{6.2.2}{78}{Output Imbalance}{section*.54}{}}
\@writefile{toc}{\contentsline {subsubsection}{Missing Values}{78}{section*.55}}
\newlabel{DEsubsec:MissingValues}{{6.2.2}{78}{Missing Values}{section*.55}{}}
\@writefile{toc}{\contentsline {subsubsection}{Feature relevance}{78}{section*.56}}
\newlabel{DEsubsec:FeatureSelection}{{6.2.2}{78}{Feature relevance}{section*.56}{}}
\citation{rokach2005clustering}
\@writefile{toc}{\contentsline {subsubsection}{Multicollinearity}{79}{section*.57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Meta-features Package}{79}{subsection.6.2.3}}
\newlabel{DEsubsec:MetaFeatures}{{6.2.3}{79}{Meta-features Package}{subsection.6.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Hypotheses}{81}{section.6.3}}
\newlabel{DEsec:Hypothesis}{{6.3}{81}{Hypotheses}{section.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Methods}{81}{section.6.4}}
\newlabel{DEsec:Methods}{{6.4}{81}{Methods}{section.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Existing Meta-features Evaluation}{81}{subsection.6.4.1}}
\newlabel{DEsubsec:MethodsExistingEvaluation}{{6.4.1}{81}{Existing Meta-features Evaluation}{subsection.6.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Additional Exploration Evaluation}{81}{subsection.6.4.2}}
\newlabel{DEsubsec:MethodsAdditionalEvaluation}{{6.4.2}{81}{Additional Exploration Evaluation}{subsection.6.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Results}{82}{section.6.5}}
\newlabel{DEsec:Results}{{6.5}{82}{Results}{section.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Existing Meta-features Evaluation}{82}{subsection.6.5.1}}
\newlabel{DEsubsec:ResultsExistingEvaluation}{{6.5.1}{82}{Existing Meta-features Evaluation}{subsection.6.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hepatitis Dataset}{82}{section*.58}}
\newlabel{DEsubsec:ExistingResultHepatitis}{{6.5.1}{82}{Hepatitis Dataset}{section*.58}{}}
\@writefile{toc}{\contentsline {subsubsection}{Micro-organisms Dataset}{84}{section*.59}}
\newlabel{DEsubsec:ExistingResultMO}{{6.5.1}{84}{Micro-organisms Dataset}{section*.59}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Additional Exploration Evaluation}{86}{subsection.6.5.2}}
\newlabel{DEsubsec:ResultsAdditionalEvaluation}{{6.5.2}{86}{Additional Exploration Evaluation}{subsection.6.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hepatitis Dataset}{86}{section*.60}}
\newlabel{DEsubsec:AdditionalResultHepatitis}{{6.5.2}{86}{Hepatitis Dataset}{section*.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces The outliers for the hepatitis dataset\relax }}{87}{table.caption.61}}
\newlabel{tab:HepatitisOutliers}{{6.1}{87}{The outliers for the hepatitis dataset\relax }{table.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The class distribution of the output of the hepatitis dataset.\relax }}{87}{figure.caption.62}}
\newlabel{fig:HepClass}{{6.1}{87}{The class distribution of the output of the hepatitis dataset.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces The boxplots of the means, standard deviations, skewnesses and kurtoses from the features of the hepatitis dataset.\relax }}{88}{figure.caption.63}}
\newlabel{fig:HepDist}{{6.2}{88}{The boxplots of the means, standard deviations, skewnesses and kurtoses from the features of the hepatitis dataset.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Histograms of the categorical features with either the highest or the lowest cardinality in the hepatitis dataset.\relax }}{89}{figure.caption.64}}
\newlabel{fig:HepCardCat}{{6.3}{89}{Histograms of the categorical features with either the highest or the lowest cardinality in the hepatitis dataset.\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Boxplots of the numeric features with either the highest or the lowest cardinality in the hepatitis dataset.\relax }}{89}{figure.caption.65}}
\newlabel{fig:HepCardNum}{{6.4}{89}{Boxplots of the numeric features with either the highest or the lowest cardinality in the hepatitis dataset.\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Histograms of the categorical features with either the highest or the lowest attribute entropy in the hepatitis dataset.\relax }}{90}{figure.caption.66}}
\newlabel{fig:HepAttEntCat}{{6.5}{90}{Histograms of the categorical features with either the highest or the lowest attribute entropy in the hepatitis dataset.\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Boxplots of the numeric features with either the highest or the lowest attribute entropy in the hepatitis dataset.\relax }}{91}{figure.caption.67}}
\newlabel{fig:HepAttEntNum}{{6.6}{91}{Boxplots of the numeric features with either the highest or the lowest attribute entropy in the hepatitis dataset.\relax }{figure.caption.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{Micro-organisms Dataset}{91}{section*.68}}
\newlabel{DEsubsec:AdditionalResultMO}{{6.5.2}{91}{Micro-organisms Dataset}{section*.68}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces The outliers for the micro-organisms dataset\relax }}{92}{table.caption.69}}
\newlabel{tab:MOOutliers}{{6.2}{92}{The outliers for the micro-organisms dataset\relax }{table.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces The class distribution of the output of the micro-organisms dataset.\relax }}{93}{figure.caption.70}}
\newlabel{fig:MOClass}{{6.7}{93}{The class distribution of the output of the micro-organisms dataset.\relax }{figure.caption.70}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Discussion}{93}{section.6.6}}
\newlabel{DEsec:Discussion}{{6.6}{93}{Discussion}{section.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces The boxplots of the means, standard deviations, skewnesses and kurtoses from the features of the micro-organisms dataset.\relax }}{94}{figure.caption.71}}
\newlabel{fig:MODist}{{6.8}{94}{The boxplots of the means, standard deviations, skewnesses and kurtoses from the features of the micro-organisms dataset.\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Boxplots of the numeric features with either the highest or the lowest cardinality in the Micro-organisms dataset.\relax }}{94}{figure.caption.72}}
\newlabel{fig:MOCardNum}{{6.9}{94}{Boxplots of the numeric features with either the highest or the lowest cardinality in the Micro-organisms dataset.\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Boxplots of the numeric features with either the highest or the lowest attribute entropy in the Micro organisms dataset.\relax }}{95}{figure.caption.73}}
\newlabel{fig:MOAttEntNum}{{6.10}{95}{Boxplots of the numeric features with either the highest or the lowest attribute entropy in the Micro organisms dataset.\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Conclusions}{95}{section.6.7}}
\newlabel{DEsec:Conclusions}{{6.7}{95}{Conclusions}{section.6.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Framework Overview}{96}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:FrameworkImplementation}{{7}{96}{Framework Overview}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{96}{section.7.1}}
\newlabel{FIsec:Introduction}{{7.1}{96}{Introduction}{section.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Dataset type}{96}{section.7.2}}
\newlabel{FIsec:DatasetType}{{7.2}{96}{Dataset type}{section.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces A graphical layout of the framework showing all aspects presented.\relax }}{97}{figure.caption.74}}
\newlabel{fig:FrameworkLayout}{{7.1}{97}{A graphical layout of the framework showing all aspects presented.\relax }{figure.caption.74}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Dataset Exploration}{98}{section.7.3}}
\newlabel{FIsec:ExplorationResults}{{7.3}{98}{Dataset Exploration}{section.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Preprocessing}{99}{section.7.4}}
\newlabel{FIsec:Preprocessing}{{7.4}{99}{Preprocessing}{section.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Automated Analysis}{100}{section.7.5}}
\newlabel{FIsec:Analysis Recommendations}{{7.5}{100}{Automated Analysis}{section.7.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusions}{101}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Conclusions}{{8}{101}{Conclusions}{chapter.8}{}}
\bibdata{../References/Citings}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Future Work}{102}{section.8.1}}
\newlabel{CCsec:Discussions}{{8.1}{102}{Future Work}{section.8.1}{}}
\bibcite{gehlenborg2010visualization}{1}
\bibcite{brazma2001minimum}{2}
\bibcite{cottrell1999probability}{3}
\bibcite{dettmer2007mass}{4}
\bibcite{liu2012data}{5}
\bibcite{sittig2008grand}{6}
\bibcite{magni1990chronic}{7}
\bibcite{bertolazzi2008logic}{8}
\bibcite{piatetsky2003microarray}{9}
\bibcite{lommen2009metalign}{10}
\bibcite{holzinger2014knowledge}{11}
\bibcite{wilkins2009proteomics}{12}
\bibcite{teodoro2009biomedical}{13}
\bibcite{doi:10.1093/nar/gkm1037}{14}
\bibcite{sturn2002genesis}{15}
\bibcite{karnovsky2011metscape}{16}
\bibcite{tabas2012genecodis3}{17}
\bibcite{faul2007g}{18}
\bibcite{baumgartner2006data}{19}
\bibcite{welthagen2005comprehensive}{20}
\bibcite{donders2006gentle}{21}
\bibcite{cartwright2003dealing}{22}
\bibcite{haukoos2007advanced}{23}
\bibcite{Deneer2017Thesis}{24}
\bibcite{nair2009genome}{25}
\bibcite{suarez2012expanding}{26}
\bibcite{bigler2013cross}{27}
\bibcite{kim2016spectrum}{28}
\bibcite{yao2008type}{29}
\bibcite{suarez2011nonlesional}{30}
\bibcite{tintle2011reversal}{31}
\bibcite{gittler2012progressive}{32}
\bibcite{CIOS20021}{33}
\bibcite{Yoo2012}{34}
\bibcite{chen2006medical}{35}
\bibcite{doi:10.1093/bib/bbx044}{36}
\bibcite{blythe2008rise}{37}
\bibcite{Turkay2014}{38}
\bibcite{Holzinger2014}{39}
\bibcite{dubitzky2007fundamentals}{40}
\bibcite{PENG201015}{41}
\bibcite{dunbar2006spatial}{42}
\bibcite{devroye1986sample}{43}
\bibcite{van2002gene}{44}
\bibcite{roff1989statistical}{45}
\bibcite{bellazzi2011data}{46}
\bibcite{Otasek2014}{47}
\bibcite{marenco2004qis}{48}
\bibcite{bichutskiy2006heterogeneous}{49}
\bibcite{sperzel1991biomedical}{50}
\bibcite{aubry1988design}{51}
\bibcite{Windridge2014}{52}
\bibcite{selvaraj2011microarray}{53}
\bibcite{afzal2015fast}{54}
\bibcite{wojnarski2010rsctc}{55}
\bibcite{matthiesen2008analysis}{56}
\bibcite{watson2007introduction}{57}
\bibcite{neves2018mass}{58}
\bibcite{NIPS2004_2728}{59}
\bibcite{doi:10.1093/bioinformatics/btu022}{60}
\bibcite{madigan2017brock}{61}
\bibcite{pocock2013clinical}{62}
\bibcite{bendele1999efficacy}{63}
\bibcite{kan1986short}{64}
\bibcite{diaconis1983computer}{65}
\bibcite{cestnikkononenkoj}{66}
\bibcite{murtaugh1994primary}{67}
\bibcite{misery2011sensitive}{68}
\bibcite{fernandes2017transfer}{69}
\bibcite{mckinney2010data}{70}
\bibcite{yan2018hands}{71}
\bibcite{walt2011NumPy}{72}
\bibcite{jones2014SciPy}{73}
\bibcite{pedregosa2011scikit}{74}
\bibcite{mckinney2012Python}{75}
\bibcite{olson2016tpot}{76}
\bibcite{yu2003feature}{77}
\bibcite{hall2000correlation}{78}
\bibcite{kohavi1995study}{79}
\bibcite{kearns1999algorithmic}{80}
\bibcite{agresti2003categorical}{81}
\bibcite{edwards2002explaining}{82}
\bibcite{han2011data}{83}
\bibcite{peng2005feature}{84}
\bibcite{battiti1994using}{85}
\bibcite{blackman2000interval}{86}
\bibcite{lim2003planar}{87}
\bibcite{peng2010novel}{88}
\bibcite{biesiada2007feature}{89}
\bibcite{ding2005minimum}{90}
\bibcite{catal2009investigating}{91}
\bibcite{liu2002comparative}{92}
\bibcite{Guyon2006}{93}
\bibcite{CATAL20091040}{94}
\bibcite{molina2002feature}{95}
\bibcite{saeys2007review}{96}
\bibcite{Duch2006}{97}
\bibcite{heiberger2004statistical}{98}
\bibcite{donoho2008higher}{99}
\bibcite{storey2003statistical}{100}
\bibcite{higgins2003measuring}{101}
\bibcite{Reunanen2006}{102}
\bibcite{Alsallakh2016PowerSet}{103}
\bibcite{tsamardinos2017massively}{104}
\bibcite{huang2013automated}{105}
\bibcite{SENAWI201747}{106}
\bibcite{el2009new}{107}
\bibcite{radovic2017minimum}{108}
\bibcite{kirkpatrick1983optimization}{109}
\bibcite{Jirapech-Umpai2005}{110}
\bibcite{Lal2006}{111}
\bibcite{blum1997selection}{112}
\bibcite{jong2004feature}{113}
\bibcite{prados2004mining}{114}
\bibcite{zhang2006recursive}{115}
\bibcite{guyon2002gene}{116}
\bibcite{geurts2005proteomic}{117}
\bibcite{wu2003comparison}{118}
\bibcite{liaw2002classification}{119}
\bibcite{thornton2013auto}{120}
\bibcite{Gijsbers2017Thesis}{121}
\bibcite{kotthoff2016auto}{122}
\bibcite{koza1997genetic}{123}
\bibcite{brazdil1994characterizing}{124}
\bibcite{vilalta2004using}{125}
\bibcite{brazdil2009development}{126}
\bibcite{pazzani1997learning}{127}
\bibcite{chen2006combining}{128}
\bibcite{hall1998practical}{129}
\bibcite{patrician2002multiple}{130}
\bibcite{sterne2009multiple}{131}
\bibcite{myrtveit2001analyzing}{132}
\bibcite{pedersen2017missing}{133}
\bibcite{raghunathan2001multivariate}{134}
\bibcite{chevret2015multiple}{135}
\bibcite{rubin1976inference}{136}
\bibcite{white2011multiple}{137}
\bibcite{he2010multiple}{138}
\bibcite{van2007multiple}{139}
\bibcite{van1999multiple}{140}
\bibcite{van2006imputation}{141}
\bibcite{azur2011multiple}{142}
\bibcite{royston2004multiple}{143}
\bibcite{martin2018impact}{144}
\bibcite{horton2001multiple}{145}
\bibcite{allison2000multiple}{146}
\bibcite{royston2011multiple}{147}
\bibcite{bartlett1935effect}{148}
\bibcite{brown1974robust}{149}
\bibcite{satorra2001scaled}{150}
\bibcite{draper2014applied}{151}
\bibcite{kluegl2009meta}{152}
\bibcite{congdon2005bayesian}{153}
\bibcite{safavian1991survey}{154}
\bibcite{gelman2006data}{155}
\bibcite{guo2016entity}{156}
\bibcite{larose2014discovering}{157}
\bibcite{al2006normalization}{158}
\bibcite{patro2015normalization}{159}
\bibcite{rokach2005clustering}{160}
\bibcite{woolson2011statistical}{161}
\bibcite{sapsford2006data}{162}
\bibcite{FISHER200993}{163}
\bibcite{dupont2009statistical}{164}
\bibcite{shiavi2010introduction}{165}
\bibcite{o2013step}{166}
\bibcite{muijs2010doing}{167}
\bibcite{muenchen2011r}{168}
\bibcite{martinez2007computational}{169}
\bibcite{BlogKromme2017}{170}
\bibcite{BlogJain2017}{171}
\bibcite{BlogWillems2014}{172}
\bibcite{BlogSupport2017}{173}
\bibcite{Goodfellow-et-al-2016}{174}
\bibcite{greenspan2016guest}{175}
\bibcite{rojas2013neural}{176}
\bibcite{Wang2003}{177}
\bibcite{bergstra2010theano}{178}
\bibcite{rampasek2016tensorflow}{179}
\bibcite{al2016theano}{180}
\bibcite{abadi2016tensorflow}{181}
\bibcite{DBLP:journals/corr/JiaSDKLGGD14}{182}
\bibcite{collobert2002torch}{183}
\bibcite{kovalev2016deep}{184}
\bibcite{bahrampour2016comparative}{185}
\bibcite{DBLP:journals/corr/ShiWXC16}{186}
\bibcite{fox2016software}{187}
\bibcite{parvat2017survey}{188}
\bibcite{erickson2017toolkits}{189}
\bibcite{rapp1999psoriasis}{190}
\bibcite{jowett1985skin}{191}
\bibcite{edgar2002gene}{192}
\bibcite{dixon2007genome}{193}
\bibcite{roberson2010psoriasis}{194}
\bibcite{leigh1995keratins}{195}
\bibcite{vegfors2012expression}{196}
\bibcite{kainu2009association}{197}
\bibcite{bergboer2012genetics}{198}
\bibcite{michibata2004identification}{199}
\bibcite{marrakchi2011interleukin}{200}
\bibcite{niehues2017psoriasis}{201}
\bibcite{jiang2015biomarkers}{202}
\bibcite{sun2010association}{203}
\bibcite{djalilian2006connexin}{204}
\bibcite{algermissen1996differential}{205}
\bibcite{bruch2003arginase}{206}
\bibcite{nestle2005plasmacytoid}{207}
\bibcite{suomela2004interferon}{208}
\bibcite{chiricozzi2011integrative}{209}
\bibcite{lowes2008psoriasis}{210}
\bibcite{jansen2009beta}{211}
\bibcite{gudjonsson2010assessment}{212}
\bibcite{roesner2017inflammatory}{213}
\bibcite{castiello2005meta}{214}
\bibcite{vilalta2002perspective}{215}
\bibcite{bourbakis2011extracting}{216}
\bibcite{wold1987principal}{217}
\bibcite{chow1968approximating}{218}
\bibcite{pfahringer2000meta}{219}
\bibstyle{ieeetr}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Abbreviations}{117}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{PLsec:Abbreviations}{{A}{117}{Abbreviations}{appendix.A}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Biomedical Data Analysis}{118}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:BiomedicalDataAnalysis}{{B}{118}{Biomedical Data Analysis}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Introduction}{118}{section.B.1}}
\citation{woolson2011statistical}
\citation{woolson2011statistical}
\citation{sapsford2006data}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Statistical Analysis}{119}{section.B.2}}
\citation{heiberger2004statistical}
\citation{FISHER200993}
\citation{woolson2011statistical}
\citation{FISHER200993}
\citation{FISHER200993}
\citation{dupont2009statistical}
\citation{FISHER200993}
\citation{FISHER200993}
\citation{dupont2009statistical}
\citation{dupont2009statistical}
\citation{shiavi2010introduction}
\citation{shiavi2010introduction}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}Statistical Research Topics}{120}{subsection.B.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Descriptive Statistics}{120}{section*.76}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Descriptive Statistics examples \cite  {FISHER200993}\relax }}{120}{table.caption.77}}
\newlabel{tab:DescrStatEx}{{B.1}{120}{Descriptive Statistics examples \cite {FISHER200993}\relax }{table.caption.77}{}}
\@writefile{toc}{\contentsline {subsubsection}{Probability Concepts}{120}{section*.80}}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces An example of a histogram of ordinal data. The depression, anxiety and stress scale of women with an acute coronary syndrome is shown. the data shows that over sixty percent rates themselves as normal. \cite  {FISHER200993}\relax }}{121}{figure.caption.78}}
\newlabel{fig:HistEx}{{B.1}{121}{An example of a histogram of ordinal data. The depression, anxiety and stress scale of women with an acute coronary syndrome is shown. the data shows that over sixty percent rates themselves as normal. \cite {FISHER200993}\relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces An example of a box plot of continuous data. The distribution of the APACHE score (Acute Physiology And Chronic Health Evaluation) was given for users of ibuprofen and placebo. The box plot doe snot only show the minimum, maximum and mean, but also quartiles and possible outliers. \cite  {dupont2009statistical}\relax }}{121}{figure.caption.79}}
\newlabel{fig:BoxEx}{{B.2}{121}{An example of a box plot of continuous data. The distribution of the APACHE score (Acute Physiology And Chronic Health Evaluation) was given for users of ibuprofen and placebo. The box plot doe snot only show the minimum, maximum and mean, but also quartiles and possible outliers. \cite {dupont2009statistical}\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces An example of a binomial distribution, a discrete probability distribution. In this example there were $n = 15$ trials with a success rate of $p = 0.4$. \cite  {heiberger2004statistical}\relax }}{121}{figure.caption.81}}
\newlabel{fig:BinomDistEx}{{B.3}{121}{An example of a binomial distribution, a discrete probability distribution. In this example there were $n = 15$ trials with a success rate of $p = 0.4$. \cite {heiberger2004statistical}\relax }{figure.caption.81}{}}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces An example of a normal distribution, a continuous probability distribution. This example is the standard normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. \cite  {heiberger2004statistical}\relax }}{122}{figure.caption.82}}
\newlabel{fig:NormDistEx}{{B.4}{122}{An example of a normal distribution, a continuous probability distribution. This example is the standard normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. \cite {heiberger2004statistical}\relax }{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces An example of a 3 student's t distributions and a normal distribution. With an sample size $n$ becoming higher, the standard deviation $s$ comes closer to normal distribution standard deviation $\sigma $. \cite  {heiberger2004statistical}\relax }}{122}{figure.caption.83}}
\newlabel{fig:StudTDistEx}{{B.5}{122}{An example of a 3 student's t distributions and a normal distribution. With an sample size $n$ becoming higher, the standard deviation $s$ comes closer to normal distribution standard deviation $\sigma $. \cite {heiberger2004statistical}\relax }{figure.caption.83}{}}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\@writefile{toc}{\contentsline {subsubsection}{Interval testing}{123}{section*.84}}
\newlabel{eq:CI}{{B.1}{123}{Interval testing}{equation.B.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Group comparisons}{123}{section*.85}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Formulas to compute whether two samples are independent by means. The parameters $s_{\Delta \mathaccentV {bar}016{y}}$ and $t_{calc}$ can be three different values (Table \ref  {tab:sdyandtcalc}) \cite  {heiberger2004statistical}. The values for the mean $\mu $, \relax }}{123}{table.caption.86}}
\newlabel{tab:IndepTest}{{B.2}{123}{Formulas to compute whether two samples are independent by means. The parameters $s_{\Delta \bar {y}}$ and $t_{calc}$ can be three different values (Table \ref {tab:sdyandtcalc}) \cite {heiberger2004statistical}. The values for the mean $\mu $, \relax }{table.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces The values of $s_{\Delta \mathaccentV {bar}016{y}}$ and $t_{calc}$ for data sets with common unknown variance, uncommon unknown variance and paired data. \cite  {heiberger2004statistical}\relax }}{123}{table.caption.87}}
\newlabel{tab:sdyandtcalc}{{B.3}{123}{The values of $s_{\Delta \bar {y}}$ and $t_{calc}$ for data sets with common unknown variance, uncommon unknown variance and paired data. \cite {heiberger2004statistical}\relax }{table.caption.87}{}}
\newlabel{eq:FVarInt}{{B.2}{123}{Group comparisons}{equation.B.2.2}{}}
\citation{heiberger2004statistical}
\citation{heiberger2004statistical}
\citation{o2013step}
\citation{muijs2010doing}
\citation{muenchen2011r}
\citation{mckinney2010data}
\citation{martinez2007computational}
\citation{BlogKromme2017}
\citation{BlogKromme2017}
\citation{BlogKromme2017}
\citation{BlogJain2017}
\citation{BlogJain2017}
\citation{BlogJain2017}
\citation{muenchen2011r}
\citation{BlogWillems2014}
\citation{BlogSupport2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.2}Statistical Programs}{125}{subsection.B.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{SAS}{125}{section*.88}}
\@writefile{toc}{\contentsline {subsubsection}{SPSS}{125}{section*.89}}
\@writefile{toc}{\contentsline {subsubsection}{R}{125}{section*.90}}
\@writefile{toc}{\contentsline {subsubsection}{Python}{125}{section*.91}}
\@writefile{toc}{\contentsline {subsubsection}{MATLAB}{125}{section*.92}}
\@writefile{toc}{\contentsline {subsubsection}{Comparisons programs}{125}{section*.93}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Advantages and Disadvantages of the statistical programs \textit  {SAS}, \textit  {SPSS}, \textit  {R} and \textit  {Python}. \cite  {BlogKromme2017}\relax }}{126}{figure.caption.94}}
\newlabel{fig:StatProgComp}{{B.6}{126}{Advantages and Disadvantages of the statistical programs \textit {SAS}, \textit {SPSS}, \textit {R} and \textit {Python}. \cite {BlogKromme2017}\relax }{figure.caption.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces Grades for \textit  {SAS}, \textit  {R} and \textit  {Python} on different aspects of using a program for statistical analysis. \cite  {BlogJain2017}\relax }}{126}{figure.caption.95}}
\newlabel{fig:StatProgGrades}{{B.7}{126}{Grades for \textit {SAS}, \textit {R} and \textit {Python} on different aspects of using a program for statistical analysis. \cite {BlogJain2017}\relax }{figure.caption.95}{}}
\@writefile{toc}{\contentsline {subsubsection}{Personal comparisons}{126}{section*.96}}
\citation{Goodfellow-et-al-2016}
\citation{greenspan2016guest}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Deep Learning}{127}{section.B.3}}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Basic Machine Learning}{128}{subsection.B.3.1}}
\newlabel{subsec:MachineLearning}{{B.3.1}{128}{Basic Machine Learning}{subsection.B.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tasks}{128}{section*.97}}
\@writefile{toc}{\contentsline {subsubsection}{Performance Measurement}{128}{section*.98}}
\@writefile{toc}{\contentsline {subsubsection}{Experience}{128}{section*.99}}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsubsection}{Challenges}{129}{section*.100}}
\citation{rojas2013neural}
\citation{rojas2013neural}
\citation{rojas2013neural}
\citation{rojas2013neural}
\citation{rojas2013neural}
\citation{rojas2013neural}
\citation{Wang2003}
\citation{Wang2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.2}Neural Networks}{130}{subsection.B.3.2}}
\newlabel{eq:NeuralModels}{{B.3}{130}{Neural Networks}{equation.B.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces An example of a basic neuron in a neural network. Input $x_1...x_n$ together with a weight $w_1...w_n$ are put into function $f$ to create output. \cite  {rojas2013neural}\relax }}{130}{figure.caption.101}}
\newlabel{fig:NeurEx}{{B.8}{130}{An example of a basic neuron in a neural network. Input $x_1...x_n$ together with a weight $w_1...w_n$ are put into function $f$ to create output. \cite {rojas2013neural}\relax }{figure.caption.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.9}{\ignorespaces An example of a neural network consisting of neurons (Figure \ref  {fig:NeurEx}). \cite  {rojas2013neural}\relax }}{130}{figure.caption.102}}
\newlabel{fig:NeurNetEx}{{B.9}{130}{An example of a neural network consisting of neurons (Figure \ref {fig:NeurEx}). \cite {rojas2013neural}\relax }{figure.caption.102}{}}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsubsection}{Convolutional Neural Networks}{131}{section*.103}}
\citation{pedregosa2011scikit}
\citation{bergstra2010theano}
\citation{rampasek2016tensorflow}
\citation{bergstra2010theano}
\citation{bergstra2010theano}
\citation{bergstra2010theano}
\citation{al2016theano}
\citation{abadi2016tensorflow}
\citation{rampasek2016tensorflow}
\citation{abadi2016tensorflow}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.3}Deep Learning Frameworks}{132}{subsection.B.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{Scikit-learn Python}{132}{section*.104}}
\@writefile{toc}{\contentsline {subsubsection}{Theano}{132}{section*.105}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.10}{\ignorespaces The steps \textit  {Theano} takes in compilation for functions for GPU. \cite  {bergstra2010theano}\relax }}{132}{figure.caption.106}}
\newlabel{fig:TheanoPipe}{{B.10}{132}{The steps \textit {Theano} takes in compilation for functions for GPU. \cite {bergstra2010theano}\relax }{figure.caption.106}{}}
\citation{abadi2016tensorflow}
\citation{rampasek2016tensorflow}
\citation{rampasek2016tensorflow}
\citation{DBLP:journals/corr/JiaSDKLGGD14}
\citation{rampasek2016tensorflow}
\citation{collobert2002torch}
\citation{kovalev2016deep}
\citation{DBLP:journals/corr/JiaSDKLGGD14}
\citation{DBLP:journals/corr/JiaSDKLGGD14}
\citation{DBLP:journals/corr/JiaSDKLGGD14}
\citation{bahrampour2016comparative}
\@writefile{toc}{\contentsline {subsubsection}{TensorFlow}{133}{section*.107}}
\@writefile{toc}{\contentsline {subsubsection}{Caffe}{133}{section*.108}}
\@writefile{toc}{\contentsline {subsubsection}{Torch}{133}{section*.109}}
\@writefile{toc}{\contentsline {subsubsection}{Comparing Frameworks}{133}{section*.110}}
\citation{DBLP:journals/corr/ShiWXC16}
\citation{bahrampour2016comparative}
\citation{fox2016software}
\citation{parvat2017survey}
\citation{erickson2017toolkits}
\@writefile{lof}{\contentsline {figure}{\numberline {B.11}{\ignorespaces The comparisons between different machine learning frameworks from \textit  {Caffe} creator point of view. \cite  {DBLP:journals/corr/JiaSDKLGGD14}\relax }}{134}{figure.caption.111}}
\newlabel{fig:CaffeMachLearnComp}{{B.11}{134}{The comparisons between different machine learning frameworks from \textit {Caffe} creator point of view. \cite {DBLP:journals/corr/JiaSDKLGGD14}\relax }{figure.caption.111}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.12}{\ignorespaces Comparisons of eight frameworks. The comparisons were implementation based.\relax }}{135}{figure.caption.112}}
\newlabel{fig:FramewComps}{{B.12}{135}{Comparisons of eight frameworks. The comparisons were implementation based.\relax }{figure.caption.112}{}}
\citation{Deneer2017Thesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Case Study 1 - Bariatric Co-morbidities}{136}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:CS1}{{C}{136}{Case Study 1 - Bariatric Co-morbidities}{appendix.C}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Introduction}{136}{section.C.1}}
\newlabel{sec:Intro}{{C.1}{136}{Introduction}{section.C.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1.1}Data Sets}{136}{subsection.C.1.1}}
\newlabel{subsec:DataSets}{{C.1.1}{136}{Data Sets}{subsection.C.1.1}{}}
\citation{Deneer2017Thesis}
\citation{Deneer2017Thesis}
\citation{Deneer2017Thesis}
\citation{Deneer2017Thesis}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1.2}Co-morbidity Marker Relation}{137}{subsection.C.1.2}}
\newlabel{subsec:CoMorMarkRel}{{C.1.2}{137}{Co-morbidity Marker Relation}{subsection.C.1.2}{}}
\newlabel{tab:DataMarkers}{{\caption@xref {tab:DataMarkers}{ on input line 3658}}{138}{Data Sets}{table.caption.113}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces The markers present in the bariatric laboratory data set \cite  {Deneer2017Thesis}\relax }}{138}{table.caption.113}}
\@writefile{lot}{\contentsline {table}{\numberline {C.2}{\ignorespaces Relations found in literature between the co-morbidities and markers. \cite  {Deneer2017Thesis}. The number shows how many citations were found that state a relation exist, nothing means no relation. A 'D' means that the disease is diagnosed from those markers.\relax }}{139}{table.caption.114}}
\newlabel{tab:CoMorMarkRel}{{C.2}{139}{Relations found in literature between the co-morbidities and markers. \cite {Deneer2017Thesis}. The number shows how many citations were found that state a relation exist, nothing means no relation. A 'D' means that the disease is diagnosed from those markers.\relax }{table.caption.114}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C.2}First Proposal}{140}{section.C.2}}
\newlabel{sec:FirstProposal}{{C.2}{140}{First Proposal}{section.C.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2.1}Research Questions}{140}{subsection.C.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2.2}Hypothesis}{140}{subsection.C.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2.3}Methods}{141}{subsection.C.2.3}}
\citation{Deneer2017Thesis}
\citation{Deneer2017Thesis}
\citation{Deneer2017Thesis}
\citation{Deneer2017Thesis}
\@writefile{toc}{\contentsline {section}{\numberline {C.3}Original research}{142}{section.C.3}}
\newlabel{sec:OriginalResearch}{{C.3}{142}{Original research}{section.C.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3.1}Preprocessing}{142}{subsection.C.3.1}}
\newlabel{subsec:Preprocessing}{{C.3.1}{142}{Preprocessing}{subsection.C.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3.2}Modelling}{142}{subsection.C.3.2}}
\newlabel{fig:FinVar}{{\caption@xref {fig:FinVar}{ on input line 3821}}{143}{Preprocessing}{figure.caption.115}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces All variables present after pre-processing \cite  {Deneer2017Thesis}\relax }}{143}{figure.caption.115}}
\newlabel{fig:FinPat}{{\caption@xref {fig:FinPat}{ on input line 3827}}{144}{Preprocessing}{figure.caption.116}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces All patients present after pre-processing \cite  {Deneer2017Thesis}\relax }}{144}{figure.caption.116}}
\@writefile{toc}{\contentsline {section}{\numberline {C.4}Proposal Comparisons}{145}{section.C.4}}
\newlabel{sec:ProposalComparisons}{{C.4}{145}{Proposal Comparisons}{section.C.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4.1}Data Omission}{145}{subsection.C.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4.2}Scoring System}{145}{subsection.C.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4.3}Modelling Tools}{146}{subsection.C.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {C.5}Final Proposal}{147}{section.C.5}}
\newlabel{sec:FinalProposal}{{C.5}{147}{Final Proposal}{section.C.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5.1}Research Questions}{147}{subsection.C.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5.2}Hypotheses}{147}{subsection.C.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5.3}Methods}{148}{subsection.C.5.3}}
\newlabel{subsec:FinalMethods}{{C.5.3}{148}{Methods}{subsection.C.5.3}{}}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{kim2016spectrum}
\citation{yao2008type}
\citation{suarez2011nonlesional}
\citation{tintle2011reversal}
\citation{gittler2012progressive}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{kim2016spectrum}
\citation{yao2008type}
\citation{suarez2011nonlesional}
\citation{tintle2011reversal}
\citation{gittler2012progressive}
\citation{rapp1999psoriasis}
\citation{jowett1985skin}
\citation{edgar2002gene}
\citation{nair2009genome}
\citation{suarez2012expanding}
\citation{bigler2013cross}
\citation{bigler2013cross}
\citation{kim2016spectrum}
\citation{yao2008type}
\citation{suarez2011nonlesional}
\citation{tintle2011reversal}
\citation{gittler2012progressive}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Case Study 2 - Skin Diseases}{150}{appendix.D}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:CS2}{{D}{150}{Case Study 2 - Skin Diseases}{appendix.D}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.1}Introduction}{150}{section.D.1}}
\newlabel{sec:Introduction}{{D.1}{150}{Introduction}{section.D.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.2}Skin Diseases Datasets}{150}{section.D.2}}
\newlabel{sec:SkinDiseasesDataSet}{{D.2}{150}{Skin Diseases Datasets}{section.D.2}{}}
\citation{dixon2007genome}
\@writefile{lot}{\contentsline {table}{\numberline {D.1}{\ignorespaces Details of the nine skin disease datasets. The number of samples and genes has been given, as well as remarks of the skin types.\relax }}{151}{table.caption.117}}
\newlabel{tab:SkinDiseasesDataSets}{{D.1}{151}{Details of the nine skin disease datasets. The number of samples and genes has been given, as well as remarks of the skin types.\relax }{table.caption.117}{}}
\citation{edgar2002gene}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2.1}Additional Data}{152}{subsection.D.2.1}}
\newlabel{subsec:AdditionalData}{{D.2.1}{152}{Additional Data}{subsection.D.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.3}Methods}{152}{section.D.3}}
\newlabel{sec:Methods}{{D.3}{152}{Methods}{section.D.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3.1}Feature Reduction}{153}{subsection.D.3.1}}
\newlabel{subsec:MethodsFeatureReduction}{{D.3.1}{153}{Feature Reduction}{subsection.D.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.1}{\ignorespaces A flowchart layout of the feature reduction. The complete data set first was reduced by using a t-test and next by the greedy clustering algorithm. The three feature set representations (all genes, significant genes and uncorrelated genes) were all three tested by a decision tree classifier and the one with the best results was chosen. At last the best feature set representation was classified with a random forest classifier and the genes that were most used in the final random forest were selected.\relax }}{153}{figure.caption.118}}
\newlabel{fig:FeatureReductionLayout}{{D.1}{153}{A flowchart layout of the feature reduction. The complete data set first was reduced by using a t-test and next by the greedy clustering algorithm. The three feature set representations (all genes, significant genes and uncorrelated genes) were all three tested by a decision tree classifier and the one with the best results was chosen. At last the best feature set representation was classified with a random forest classifier and the genes that were most used in the final random forest were selected.\relax }{figure.caption.118}{}}
\newlabel{eq:PairedTTest}{{D.1}{153}{Feature Reduction}{equation.D.3.1}{}}
\newlabel{eq:UnequalVarianceTTest}{{D.2}{153}{Feature Reduction}{equation.D.3.2}{}}
\newlabel{fn:BDA}{{2}{153}{Feature Reduction}{figure.caption.118}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {19}{\ignorespaces The greedy multicollinearity removal algorithm\relax }}{154}{algorithm.19}}
\newlabel{alg:GreedyCorrelationRemoval}{{19}{154}{The greedy multicollinearity removal algorithm\relax }{algorithm.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.2}{\ignorespaces A flowchart layout how the gene multicollinearity removal algorithm worked. The correlation coefficient is iteratively computed between a new gene and the uncorrelated set of genes. If all correlation coefficients are lower than $0.7$, the new gene did not correlate enough with the set of genes and therefore is added to the uncorrelated set of genes. If it had a correlation of higher than 0.7 for another gene, it was added to that the list for the gene it is correlated with.\relax }}{154}{figure.caption.119}}
\newlabel{fig:GreedyCorrelationClustering}{{D.2}{154}{A flowchart layout how the gene multicollinearity removal algorithm worked. The correlation coefficient is iteratively computed between a new gene and the uncorrelated set of genes. If all correlation coefficients are lower than $0.7$, the new gene did not correlate enough with the set of genes and therefore is added to the uncorrelated set of genes. If it had a correlation of higher than 0.7 for another gene, it was added to that the list for the gene it is correlated with.\relax }{figure.caption.119}{}}
\newlabel{fig:DTFeatureSpace}{{D.3a}{155}{Subfigure D D.3a}{subfigure.D.3.1}{}}
\newlabel{sub@fig:DTFeatureSpace}{{(a)}{a}{Subfigure D D.3a\relax }{subfigure.D.3.1}{}}
\newlabel{fig:DTExample}{{D.3b}{155}{Subfigure D D.3b}{subfigure.D.3.2}{}}
\newlabel{sub@fig:DTExample}{{(b)}{b}{Subfigure D D.3b\relax }{subfigure.D.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.3}{\ignorespaces An example of how a decision tree divides a sample space in different regions. A sample space (Figure \ref  {fig:DTFeatureSpace}) with features ($x_1$ and $x_2$) is divided in regions ($\omega _1$, $\omega _1$ and $\omega _1$). A decision tree (Figure \ref  {fig:DTExample}) shows how this division is done using thresholds for the features. Every node in the tree corresponds to dividing the sample space in different regions using a threshold for a feature and every leaf corresponds to a region. The classifier tries to divide the sample space in regions finding the best feature threshold combinations, until boundary conditions are met.\relax }}{155}{figure.caption.120}}
\newlabel{fig:DecisionTree}{{D.3}{155}{An example of how a decision tree divides a sample space in different regions. A sample space (Figure \ref {fig:DTFeatureSpace}) with features ($x_1$ and $x_2$) is divided in regions ($\omega _1$, $\omega _1$ and $\omega _1$). A decision tree (Figure \ref {fig:DTExample}) shows how this division is done using thresholds for the features. Every node in the tree corresponds to dividing the sample space in different regions using a threshold for a feature and every leaf corresponds to a region. The classifier tries to divide the sample space in regions finding the best feature threshold combinations, until boundary conditions are met.\relax }{figure.caption.120}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A two dimensional sample space divided in three regions.}}}{155}{subfigure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The corresponding decision tree}}}{155}{subfigure.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3.2}Clustering}{155}{subsection.D.3.2}}
\newlabel{subsec:MethodsPositionalClustering}{{D.3.2}{155}{Clustering}{subsection.D.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.4}{\ignorespaces A flowchart layout how clustering was done. Two types of clustering were used, by biomedical relation and by value. For the clustering by value, first a normalization was done. After both types of clustering were done, they were combined and biomedical relations within the clusters were shown.\relax }}{156}{figure.caption.121}}
\newlabel{fig:ClusteringLayout}{{D.4}{156}{A flowchart layout how clustering was done. Two types of clustering were used, by biomedical relation and by value. For the clustering by value, first a normalization was done. After both types of clustering were done, they were combined and biomedical relations within the clusters were shown.\relax }{figure.caption.121}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.5}{\ignorespaces An explanation of the difference between normalization per feature and normalization per sample. $x_{a,b}$ is value $x$ for sample $a$ and feature $b$, mean() gives the mean of the values given (either a sample row or feature column) and std() gives the standard deviation for the values given. For both the formula of how to compute the normalized value $x'_{a,b}$ is given.\relax }}{157}{figure.caption.122}}
\newlabel{fig:Normalization}{{D.5}{157}{An explanation of the difference between normalization per feature and normalization per sample. $x_{a,b}$ is value $x$ for sample $a$ and feature $b$, mean() gives the mean of the values given (either a sample row or feature column) and std() gives the standard deviation for the values given. For both the formula of how to compute the normalized value $x'_{a,b}$ is given.\relax }{figure.caption.122}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.2}{\ignorespaces The three different clustering types for clustering by value.\relax }}{157}{table.caption.123}}
\newlabel{tab:ClusteringTypes}{{D.2}{157}{The three different clustering types for clustering by value.\relax }{table.caption.123}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.3}{\ignorespaces The three biomedical relation types available in the database (subsection \ref  {subsec:AdditionalData}) with a description and several examples.\relax }}{158}{table.caption.124}}
\newlabel{tab:BiomedicalRelationTypes}{{D.3}{158}{The three biomedical relation types available in the database (subsection \ref {subsec:AdditionalData}) with a description and several examples.\relax }{table.caption.124}{}}
\newlabel{eq:ProcessCoverage}{{D.3}{158}{Clustering}{equation.D.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3.3}Psoriasis Versus Atopic Dermatitis}{158}{subsection.D.3.3}}
\newlabel{subsec:MethodsPsoriasisVersusAtopicDermatitis}{{D.3.3}{158}{Psoriasis Versus Atopic Dermatitis}{subsection.D.3.3}{}}
\citation{suarez2012expanding}
\citation{suarez2012expanding}
\citation{suarez2012expanding}
\citation{suarez2012expanding}
\citation{suarez2012expanding}
\citation{suarez2012expanding}
\@writefile{toc}{\contentsline {section}{\numberline {D.4}Results}{159}{section.D.4}}
\newlabel{sec:Results}{{D.4}{159}{Results}{section.D.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4.1}Feature Reduction}{159}{subsection.D.4.1}}
\newlabel{subsec:ResultsFeatureResuction}{{D.4.1}{159}{Feature Reduction}{subsection.D.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.4}{\ignorespaces The results of using the t-test for genes in all relevant psoriasis datasets separately and combined. A paired t-test is done when they are paired, otherwise an unknown variance unpaired t-test was done. The number of samples for both lesional and non-lesional skin are shown additionally.\relax }}{159}{table.caption.125}}
\newlabel{tab:FeatureReduction}{{D.4}{159}{The results of using the t-test for genes in all relevant psoriasis datasets separately and combined. A paired t-test is done when they are paired, otherwise an unknown variance unpaired t-test was done. The number of samples for both lesional and non-lesional skin are shown additionally.\relax }{table.caption.125}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.5}{\ignorespaces The results of the decision tree classifier for different sets of genes: all genes, only the significant genes and the uncorrelated genes. The decision tree is cross validated by division in 100 different subsets and afterwards tested by a separate test set.\relax }}{159}{table.caption.126}}
\newlabel{tab:DecisionTreeResults}{{D.5}{159}{The results of the decision tree classifier for different sets of genes: all genes, only the significant genes and the uncorrelated genes. The decision tree is cross validated by division in 100 different subsets and afterwards tested by a separate test set.\relax }{table.caption.126}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.6}{\ignorespaces The six most used genes as splitting criteria for the random forest classifier. The genes are also compared with literature.\relax }}{160}{table.caption.127}}
\newlabel{tab:GeneRandomForestOccurence}{{D.6}{160}{The six most used genes as splitting criteria for the random forest classifier. The genes are also compared with literature.\relax }{table.caption.127}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4.2}Clustering}{160}{subsection.D.4.2}}
\newlabel{subsec:ResultsPositionalClustering}{{D.4.2}{160}{Clustering}{subsection.D.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.6}{\ignorespaces Four examples of processes that show a high difference in gene expression between lesional and non-lesional data. The data is normalized per gene and orange points are genes that are related to the process, blue points are genes unrelated to the process\relax }}{161}{figure.caption.128}}
\newlabel{fig:ProcessesScaledBySample}{{D.6}{161}{Four examples of processes that show a high difference in gene expression between lesional and non-lesional data. The data is normalized per gene and orange points are genes that are related to the process, blue points are genes unrelated to the process\relax }{figure.caption.128}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.7}{\ignorespaces 10 different clusters found by agglomerative clustering for the Psoriasis dataset. The data points are genes that show a significant difference in expression between lesional (y-axis) and non-lesional (x-axis) skin and normalized per sample. Cluster 5 and 8 show the most useful genes and its genes are also marked for Psoriasis relevance (Appendix \ref  {app:ClusterGeneSpecifications}): Known Psoriasis marker (A), Known to be up-regulated for Psoriasis (B), Known to be up-regulated for uncontrollable growth (C) and unknown relation (D)\relax }}{162}{figure.caption.129}}
\newlabel{fig:ClusteredGenes}{{D.7}{162}{10 different clusters found by agglomerative clustering for the Psoriasis dataset. The data points are genes that show a significant difference in expression between lesional (y-axis) and non-lesional (x-axis) skin and normalized per sample. Cluster 5 and 8 show the most useful genes and its genes are also marked for Psoriasis relevance (Appendix \ref {app:ClusterGeneSpecifications}): Known Psoriasis marker (A), Known to be up-regulated for Psoriasis (B), Known to be up-regulated for uncontrollable growth (C) and unknown relation (D)\relax }{figure.caption.129}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4.3}Psoriasis Versus Atopic Dermatitis}{162}{subsection.D.4.3}}
\newlabel{subsec:ResultsPsoriasisVersusAtopicDermatitis}{{D.4.3}{162}{Psoriasis Versus Atopic Dermatitis}{subsection.D.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.7}{\ignorespaces The initial conditions of the Psoriasis and Atopic Dermatitis datasets\relax }}{163}{table.caption.130}}
\newlabel{tab:ConditionsPsoriasisAD}{{D.7}{163}{The initial conditions of the Psoriasis and Atopic Dermatitis datasets\relax }{table.caption.130}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.5}Discussion}{163}{section.D.5}}
\newlabel{sec:Conclusion}{{D.5}{163}{Discussion}{section.D.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.6}Conclusion for Framework}{163}{section.D.6}}
\newlabel{sec:DiscussionForFramework}{{D.6}{163}{Conclusion for Framework}{section.D.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.7}Appendix: Biomedical relation graphs}{164}{section.D.7}}
\newlabel{app:BiomedicalClusters}{{D.7}{164}{Appendix: Biomedical relation graphs}{section.D.7}{}}
\newlabel{fig:ProcessesScaledByFeature}{{\caption@xref {fig:ProcessesScaledByFeature}{ on input line 4385}}{164}{Appendix: Biomedical relation graphs}{figure.caption.131}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.8}{\ignorespaces The 16 processes that showed the highest difference in gene expression between lesional and non-lesional data. The data is normalized per feature and orange points are genes that are related to the process, Blue dots are genes unrelated to the process. The band between the two separate distributes is created by removal of the insignificant genes.\relax }}{164}{figure.caption.131}}
\newlabel{fig:CellularScaledBySample}{{\caption@xref {fig:CellularScaledBySample}{ on input line 4391}}{165}{Appendix: Biomedical relation graphs}{figure.caption.132}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.9}{\ignorespaces The 16 cellular locations that showed the highest difference in gene expression between lesional and non-lesional data. The data is normalized per sample and orange points are genes that are related to the cellular location, Blue dots are genes unrelated to the cellular location.\relax }}{165}{figure.caption.132}}
\newlabel{fig:CellularScaledByFeature1}{{\caption@xref {fig:CellularScaledByFeature1}{ on input line 4398}}{165}{Appendix: Biomedical relation graphs}{figure.caption.133}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.10}{\ignorespaces The 16 cellular locations that showed the highest difference in gene expression between lesional and non-lesional data. The data is normalized per feature and orange points are genes that are related to the cellular location, Blue dots are genes unrelated to the cellular location. The band between the two separate distributes is created by removal of the insignificant genes.\relax }}{165}{figure.caption.133}}
\newlabel{fig:MolecularScaledBySample}{{\caption@xref {fig:MolecularScaledBySample}{ on input line 4404}}{166}{Appendix: Biomedical relation graphs}{figure.caption.134}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.11}{\ignorespaces The 16 molecular relations that showed the highest difference in gene expression between lesional and non-lesional data. The data is normalized per sample and orange points are genes that are related to the molecular relation, Blue dots are genes unrelated to the molecular relation.\relax }}{166}{figure.caption.134}}
\newlabel{fig:MolecularScaledByFeature1}{{\caption@xref {fig:MolecularScaledByFeature1}{ on input line 4411}}{166}{Appendix: Biomedical relation graphs}{figure.caption.135}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.12}{\ignorespaces The 16 molecular reactions that showed the highest difference in gene expression between lesional and non-lesional data. The data is normalized per feature and orange points are genes that are related to the molecular relation, Blue dots are genes unrelated to the molecular relation. The band between the two separate distributes is created by removal of the insignificant genes.\relax }}{166}{figure.caption.135}}
\citation{roberson2010psoriasis}
\citation{leigh1995keratins}
\citation{vegfors2012expression}
\citation{kainu2009association}
\citation{bergboer2012genetics}
\citation{michibata2004identification}
\citation{marrakchi2011interleukin}
\citation{niehues2017psoriasis}
\citation{roberson2010psoriasis}
\citation{jiang2015biomarkers}
\citation{roberson2010psoriasis}
\citation{bergboer2012genetics}
\citation{jiang2015biomarkers}
\citation{jiang2015biomarkers}
\citation{suarez2012expanding}
\citation{sun2010association}
\citation{djalilian2006connexin}
\citation{algermissen1996differential}
\citation{bruch2003arginase}
\citation{nestle2005plasmacytoid}
\citation{suomela2004interferon}
\citation{bergboer2012genetics}
\citation{suarez2012expanding}
\citation{chiricozzi2011integrative}
\citation{lowes2008psoriasis}
\citation{jiang2015biomarkers}
\citation{chiricozzi2011integrative}
\citation{lowes2008psoriasis}
\citation{jiang2015biomarkers}
\citation{lowes2008psoriasis}
\citation{jiang2015biomarkers}
\citation{jansen2009beta}
\citation{gudjonsson2010assessment}
\citation{roesner2017inflammatory}
\citation{gudjonsson2010assessment}
\citation{roesner2017inflammatory}
\@writefile{toc}{\contentsline {section}{\numberline {D.8}Appendix: Cluster Gene Specifications}{167}{section.D.8}}
\newlabel{app:ClusterGeneSpecifications}{{D.8}{167}{Appendix: Cluster Gene Specifications}{section.D.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.8}{\ignorespaces All genes in the highlighted cluster 5 with their link to psoriasis\relax }}{168}{table.caption.136}}
\newlabel{tab:GenesCluster5}{{D.8}{168}{All genes in the highlighted cluster 5 with their link to psoriasis\relax }{table.caption.136}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.9}{\ignorespaces All genes in the highlighted cluster 8 with their link to psoriasis\relax }}{169}{table.caption.137}}
\newlabel{tab:GenesCluster8}{{D.9}{169}{All genes in the highlighted cluster 8 with their link to psoriasis\relax }{table.caption.137}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {E}Feature Selection Results}{170}{appendix.E}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {E.1}Feature Selection Exploration Plots}{170}{section.E.1}}
\newlabel{app:FeatureSelectionExplorationPlots}{{E.1}{170}{Feature Selection Exploration Plots}{section.E.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.1}{\ignorespaces The validation and test score per feature preserved for the Micro-organisms data set with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{170}{figure.caption.138}}
\newlabel{fig:MO_MI_Val_Test_Score}{{E.1}{170}{The validation and test score per feature preserved for the Micro-organisms data set with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.138}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.2}{\ignorespaces The validation and test score per feature preserved for the Micro-organisms dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{171}{figure.caption.139}}
\newlabel{fig:MO_T_Val_Test_Score}{{E.2}{171}{The validation and test score per feature preserved for the Micro-organisms dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.139}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.3}{\ignorespaces The validation and test score per feature preserved for the Arcene dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{171}{figure.caption.140}}
\newlabel{fig:Arcene_MI_Val_Test_Score}{{E.3}{171}{The validation and test score per feature preserved for the Arcene dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.140}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.4}{\ignorespaces The validation and test score per feature preserved for the Arcene dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{172}{figure.caption.141}}
\newlabel{fig:Arcene_T_Val_Test_Score}{{E.4}{172}{The validation and test score per feature preserved for the Arcene dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.141}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.5}{\ignorespaces The validation and test score per feature preserved for the RSCTC dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{172}{figure.caption.142}}
\newlabel{fig:RSCTC_MI_Val_Test_Score}{{E.5}{172}{The validation and test score per feature preserved for the RSCTC dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.142}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.6}{\ignorespaces The validation and test score per feature preserved for the RSCTC dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{173}{figure.caption.143}}
\newlabel{fig:RSCTC_T_Val_Test_Score}{{E.6}{173}{The validation and test score per feature preserved for the RSCTC dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.143}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.7}{\ignorespaces The validation and test score per feature preserved for the Psoriasis dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }}{173}{figure.caption.144}}
\newlabel{fig:Psoriasis_MI_Val_Test_Score}{{E.7}{173}{The validation and test score per feature preserved for the Psoriasis dataset with ranking method mutual information. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.144}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.8}{\ignorespaces The validation and test score per feature preserved for the Psoriasis dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }}{174}{figure.caption.145}}
\newlabel{fig:Psoriasis_T_Val_Test_Score}{{E.8}{174}{The validation and test score per feature preserved for the Psoriasis dataset with ranking method T-test. Five different classification algorithms are used to define this classification and test score.\relax }{figure.caption.145}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E.2}Feature Selection Exploration Precision And Recall}{174}{section.E.2}}
\newlabel{app:PrecisionRecall}{{E.2}{174}{Feature Selection Exploration Precision And Recall}{section.E.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.9}{\ignorespaces The average precision shown per dataset and rank.\relax }}{174}{figure.caption.146}}
\newlabel{fig:DatasetRankPrecisionScores}{{E.9}{174}{The average precision shown per dataset and rank.\relax }{figure.caption.146}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.10}{\ignorespaces The average recall shown per dataset and rank.\relax }}{175}{figure.caption.147}}
\newlabel{fig:DatasetRankRecallScores}{{E.10}{175}{The average recall shown per dataset and rank.\relax }{figure.caption.147}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E.3}Feature Selection Evaluation Plots}{175}{section.E.3}}
\newlabel{app:EvaluationPlots}{{E.3}{175}{Feature Selection Evaluation Plots}{section.E.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.11}{\ignorespaces The precision spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the precision of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{176}{figure.caption.148}}
\newlabel{fig:Avg_Precision_Spectrum}{{E.11}{176}{The precision spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the precision of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.148}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.12}{\ignorespaces The recall spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the recall of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }}{177}{figure.caption.149}}
\newlabel{fig:Avg_Recall_Spectrum}{{E.12}{177}{The recall spectrum for the average dataset. The x-axis shows the average number of features that are preserved and the y-axis shows the recall of logistic regression. The legend indicates the algorithms and their corresponding shapes, as well as the chosen parameters with their matching colours. Abbreviations in legend: Mutual Information (MI), Pick l-Take Away r (PTA), Machine Learning algorithm (ML), Support Vector Machine (svm), random forest (rf)\relax }{figure.caption.149}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {F}Missing Value Handling Results}{178}{appendix.F}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {F.1}Feature Distribution Tables}{178}{section.F.1}}
\newlabel{app:DistributionTables}{{F.1}{178}{Feature Distribution Tables}{section.F.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {F.3}{\ignorespaces Testing for the cirrhosis dataset whether the type of missing values can be represented by the remaining values. This is done by comparing distributions between all sample values and remaining sample values after CCA and WCA. For numeric values, two tests were used, an independent t-test for equality of mean and a Levene's test with the median in brackets to test equality of variance. For ordinal and categorical features the medians and modes where checked respectively to be similar as well as a chi squared test in brackets fro equality of distribution. P-values lower than $p < 0.05$ are marked red for failure of representation, p-values higher than $p > 0.05$ are marked green for correctly being represented. If at least one feature is not represented after CCA, the missing values cannot be MCAR. If at least one feature is not represented after WCA, the pseudo-randomness cannot be corrected by only using weights for other values.\relax }}{179}{table.caption.152}}
\newlabel{tab:LDCirrhosis}{{F.3}{179}{Testing for the cirrhosis dataset whether the type of missing values can be represented by the remaining values. This is done by comparing distributions between all sample values and remaining sample values after CCA and WCA. For numeric values, two tests were used, an independent t-test for equality of mean and a Levene's test with the median in brackets to test equality of variance. For ordinal and categorical features the medians and modes where checked respectively to be similar as well as a chi squared test in brackets fro equality of distribution. P-values lower than $p < 0.05$ are marked red for failure of representation, p-values higher than $p > 0.05$ are marked green for correctly being represented. If at least one feature is not represented after CCA, the missing values cannot be MCAR. If at least one feature is not represented after WCA, the pseudo-randomness cannot be corrected by only using weights for other values.\relax }{table.caption.152}{}}
\@writefile{lot}{\contentsline {table}{\numberline {F.4}{\ignorespaces Testing for the cirrhosis data set if certain types of imputation create a vastly different distribution for features with missing values. The imputation values are generated with mean imputation, hot deck imputation, k-Nearest Neighbour imputation ($k = 3$), regression imputation and MICE (number of cycles is $s = 5$). For numeric values, two tests were used, an independent t-test for equality of mean and a Levene's test with the median in brackets to test equality of variance. For ordinal and categorical features the medians and modes where checked respectively to be similar as well as a chi squared test in brackets fro equality of distribution. P-values lower than $p < 0.05$ are marked red for failure of representation, p-values close to $p > 0.05$ are marked green for correctly being represented.\relax }}{180}{table.caption.153}}
\newlabel{tab:ImputationCirrhosis}{{F.4}{180}{Testing for the cirrhosis data set if certain types of imputation create a vastly different distribution for features with missing values. The imputation values are generated with mean imputation, hot deck imputation, k-Nearest Neighbour imputation ($k = 3$), regression imputation and MICE (number of cycles is $s = 5$). For numeric values, two tests were used, an independent t-test for equality of mean and a Levene's test with the median in brackets to test equality of variance. For ordinal and categorical features the medians and modes where checked respectively to be similar as well as a chi squared test in brackets fro equality of distribution. P-values lower than $p < 0.05$ are marked red for failure of representation, p-values close to $p > 0.05$ are marked green for correctly being represented.\relax }{table.caption.153}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F.2}Classification Plots}{180}{section.F.2}}
\newlabel{app:ClassPlots}{{F.2}{180}{Classification Plots}{section.F.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.1}{\ignorespaces A plot showing the accuracy for the Hepatitis dataset after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }}{181}{figure.caption.155}}
\newlabel{fig:EvalHepaAcc}{{F.1}{181}{A plot showing the accuracy for the Hepatitis dataset after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }{figure.caption.155}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.2}{\ignorespaces A plot showing the accuracy for the Cirrhosis dataset after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }}{182}{figure.caption.156}}
\newlabel{fig:EvalCirrhAcc}{{F.2}{182}{A plot showing the accuracy for the Cirrhosis dataset after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }{figure.caption.156}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.3}{\ignorespaces A plot showing the accuracy for the Cervical cancer dataset after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }}{183}{figure.caption.157}}
\newlabel{fig:EvalCervhAcc}{{F.3}{183}{A plot showing the accuracy for the Cervical cancer dataset after using a missing value handling algorithm. The x-axis shows the computation time for the missing values, done on a 10-logarithmic scale due to the big differences between them. The y-axis shows the accuracy on a scale of 0 to 1. The explanation of every data point can be found both in the text on the top right of the data point as well as in the legend.\relax }{figure.caption.157}{}}
\citation{kluegl2009meta}
\citation{kluegl2009meta}
\citation{castiello2005meta}
\citation{vilalta2002perspective}
\citation{bourbakis2011extracting}
\@writefile{toc}{\contentsline {chapter}{\numberline {G}Dataset Exploration Meta-features}{184}{appendix.G}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {G.1}Meta-features}{184}{section.G.1}}
\newlabel{app:MetaFeatures}{{G.1}{184}{Meta-features}{section.G.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.1.1}Basic Meta-features}{184}{subsection.G.1.1}}
\newlabel{subsec:BasicMF}{{G.1.1}{184}{Basic Meta-features}{subsection.G.1.1}{}}
\citation{wold1987principal}
\citation{yu2003feature}
\citation{hall2000correlation}
\citation{agresti2003categorical}
\citation{chow1968approximating}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.1.2}Statistical Meta-features}{185}{subsection.G.1.2}}
\newlabel{subsec:StatisticalMF}{{G.1.2}{185}{Statistical Meta-features}{subsection.G.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.1.3}Information-theoretic Meta-features}{185}{subsection.G.1.3}}
\newlabel{subsec:InformationTheoreticMF}{{G.1.3}{185}{Information-theoretic Meta-features}{subsection.G.1.3}{}}
\newlabel{eq:Entropy}{{G.1}{185}{Information-theoretic Meta-features}{equation.G.1.1}{}}
\citation{pfahringer2000meta}
\newlabel{eq:EqNumFeat}{{G.2}{186}{Information-theoretic Meta-features}{equation.G.1.2}{}}
\newlabel{eq:NoiseToSignal}{{G.3}{186}{Information-theoretic Meta-features}{equation.G.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.1.4}Landmarking Meta-features}{186}{subsection.G.1.4}}
\newlabel{subsec:LandmarkingMF}{{G.1.4}{186}{Landmarking Meta-features}{subsection.G.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {G.2}Meta-feature Values Hepatitis}{186}{section.G.2}}
\newlabel{app:MetafeatureValuesHepatitis}{{G.2}{186}{Meta-feature Values Hepatitis}{section.G.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.1}{\ignorespaces The cardinality meta-features for the Hepatitis dataset\relax }}{187}{table.caption.158}}
\newlabel{tab:HepatitisCardinalities}{{G.1}{187}{The cardinality meta-features for the Hepatitis dataset\relax }{table.caption.158}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.2}{\ignorespaces The numeric feature distribution of the hepatitis dataset\relax }}{187}{table.caption.159}}
\newlabel{tab:HepatitisFeatureDistributions}{{G.2}{187}{The numeric feature distribution of the hepatitis dataset\relax }{table.caption.159}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.3}{\ignorespaces The entropies and mutual information values of the hepatitis dataset\relax }}{188}{table.caption.160}}
\newlabel{tab:EntropyMIHepatitisDataset}{{G.3}{188}{The entropies and mutual information values of the hepatitis dataset\relax }{table.caption.160}{}}
\@writefile{toc}{\contentsline {section}{\numberline {G.3}Meta-feature Values Micro-Organisms}{188}{section.G.3}}
\newlabel{app:MetafeatureValuesMO}{{G.3}{188}{Meta-feature Values Micro-Organisms}{section.G.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.4}{\ignorespaces The landmarking meta-features of the hepatitis dataset\relax }}{189}{table.caption.161}}
\newlabel{tab:LandmarkingHepatitis}{{G.4}{189}{The landmarking meta-features of the hepatitis dataset\relax }{table.caption.161}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.5}{\ignorespaces The cardinality meta-features for the Micro-organisms dataset\relax }}{190}{table.caption.162}}
\newlabel{tab:MOCardinalities}{{G.5}{190}{The cardinality meta-features for the Micro-organisms dataset\relax }{table.caption.162}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.6}{\ignorespaces The numeric feature distribution of the Micro-organisms dataset\relax }}{190}{table.caption.163}}
\newlabel{tab:MOFeatureDistributions}{{G.6}{190}{The numeric feature distribution of the Micro-organisms dataset\relax }{table.caption.163}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.7}{\ignorespaces The entropies and mutual information values of the Micro-organisms dataset\relax }}{190}{table.caption.164}}
\newlabel{tab:EntropyMIMODataset}{{G.7}{190}{The entropies and mutual information values of the Micro-organisms dataset\relax }{table.caption.164}{}}
\@writefile{lot}{\contentsline {table}{\numberline {G.8}{\ignorespaces The landmarking meta-features of the Micro-organisms dataset\relax }}{191}{table.caption.165}}
\newlabel{tab:LandmarkingMO}{{G.8}{191}{The landmarking meta-features of the Micro-organisms dataset\relax }{table.caption.165}{}}
